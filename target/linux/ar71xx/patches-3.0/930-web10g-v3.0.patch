diff --git a/include/linux/sysctl.h b/include/linux/sysctl.h
index 11684d9..da0ed78 100644
--- a/include/linux/sysctl.h
+++ b/include/linux/sysctl.h
@@ -983,6 +983,11 @@ extern int proc_doulongvec_ms_jiffies_minmax(struct ctl_table *table, int,
 				      void __user *, size_t *, loff_t *);
 extern int proc_do_large_bitmap(struct ctl_table *, int,
 				void __user *, size_t *, loff_t *);
+#ifdef CONFIG_TCP_ESTATS
+extern int tcp_estats_proc_dointvec_update(ctl_table *, int, void *, 
+					   size_t *, loff_t *);
+#endif
+
 
 /*
  * Register a set of sysctl names by calling register_sysctl_table
diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index e64f4c6..76a434c 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -255,6 +255,9 @@ struct tcp_options_received {
 		cookie_in_always:1;
 	u8	num_sacks;	/* Number of SACK blocks		*/
 	u16	user_mss;	/* mss requested by user in ioctl	*/
+#ifdef CONFIG_TCP_ESTATS
+	u16	rec_mss;	/* MSS option received */
+#endif
 	u16	mss_clamp;	/* Maximal mss, negotiated at connection setup */
 };
 
@@ -289,6 +292,10 @@ static inline struct tcp_request_sock *tcp_rsk(const struct request_sock *req)
 	return (struct tcp_request_sock *)req;
 }
 
+#ifdef CONFIG_TCP_ESTATS
+struct tcp_estats;
+#endif
+
 struct tcp_sock {
 	/* inet_connection_sock has to be the first member of tcp_sock */
 	struct inet_connection_sock	inet_conn;
@@ -460,6 +467,10 @@ struct tcp_sock {
 	 * contains related tcp_cookie_transactions fields.
 	 */
 	struct tcp_cookie_values  *cookie_values;
+
+#ifdef CONFIG_TCP_ESTATS
+	struct tcp_estats	*tcp_stats;
+#endif
 };
 
 static inline struct tcp_sock *tcp_sk(const struct sock *sk)
diff --git a/include/net/tcp.h b/include/net/tcp.h
index cda30ea..ea0316b 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -43,6 +43,7 @@
 #include <net/tcp_states.h>
 #include <net/inet_ecn.h>
 #include <net/dst.h>
+#include <net/tcp_estats.h>
 
 #include <linux/seq_file.h>
 
@@ -246,6 +247,10 @@ extern int sysctl_tcp_max_ssthresh;
 extern int sysctl_tcp_cookie_size;
 extern int sysctl_tcp_thin_linear_timeouts;
 extern int sysctl_tcp_thin_dupack;
+#ifdef CONFIG_TCP_ESTATS
+extern int sysctl_tcp_estats_fperms;
+extern int sysctl_tcp_estats_gid;
+#endif
 
 extern atomic_long_t tcp_memory_allocated;
 extern struct percpu_counter tcp_sockets_allocated;
diff --git a/include/net/tcp_estats.h b/include/net/tcp_estats.h
new file mode 100644
index 0000000..80c2b2f
--- /dev/null
+++ b/include/net/tcp_estats.h
@@ -0,0 +1,372 @@
+/*
+ * include/net/tcp_estats.h
+ *
+ * Implementation of TCP ESTATS MIB (RFC 4898)
+ *
+ * Authors:
+ *   John Heffner <jheffner@psc.edu>
+ *   Matt Mathis <mathis@psc.edu>
+ *   Jeff Semke <semke@psc.edu>
+ *
+ * The Web10Gig project.  See http://www.web10gig.org
+ *
+ * Copyright Â© 2011, Pittsburgh Supercomputing Center (PSC).
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ *
+ */
+
+#ifndef _TCP_ESTATS_H
+#define _TCP_ESTATS_H
+
+#include <net/sock.h>
+#include <linux/tcp.h>
+#include <linux/spinlock.h>
+
+enum tcp_estats_sndlim_states {
+	TCP_ESTATS_SNDLIM_NONE = -1,
+	TCP_ESTATS_SNDLIM_SENDER,
+	TCP_ESTATS_SNDLIM_CWND,
+	TCP_ESTATS_SNDLIM_RWIN,
+	TCP_ESTATS_SNDLIM_STARTUP,
+	TCP_ESTATS_SNDLIM_NSTATES	/* Keep at end */
+};
+
+
+enum tcp_estats_addrtype {
+	TCP_ESTATS_ADDRTYPE_IPV4 = 1,
+	TCP_ESTATS_ADDRTYPE_IPV6 = 2
+};
+
+#ifdef CONFIG_TCP_ESTATS
+#define TCP_ESTATS_CHECK(tp,expr) \
+	do { if ((tp)->tcp_stats) (expr); } while (0)
+#define TCP_ESTATS_VAR_INC(tp,var) \
+	TCP_ESTATS_CHECK(tp, ((tp)->tcp_stats->estats_vars.var)++)
+#define TCP_ESTATS_VAR_DEC(tp,var) \
+	TCP_ESTATS_CHECK(tp, ((tp)->tcp_stats->estats_vars.var)--)
+#define TCP_ESTATS_VAR_ADD(tp,var,val) \
+	TCP_ESTATS_CHECK(tp, ((tp)->tcp_stats->estats_vars.var) += (val))
+#define TCP_ESTATS_VAR_SET(tp,var,val) \
+	TCP_ESTATS_CHECK(tp, ((tp)->tcp_stats->estats_vars.var) = (val))
+#define TCP_ESTATS_UPDATE(tp,func) \
+	TCP_ESTATS_CHECK(tp, func)
+
+/* The official MIB states are enumerated differently than
+ * Linux's.  Use tcp_estats_state() to convert. */
+enum tcp_estats_states {
+	TCP_ESTATS_STATE_CLOSED = 1,
+	TCP_ESTATS_STATE_LISTEN,
+	TCP_ESTATS_STATE_SYNSENT,
+	TCP_ESTATS_STATE_SYNRECEIVED,
+	TCP_ESTATS_STATE_ESTABLISHED,
+	TCP_ESTATS_STATE_FINWAIT1,
+	TCP_ESTATS_STATE_FINWAIT2,
+	TCP_ESTATS_STATE_CLOSEWAIT,
+	TCP_ESTATS_STATE_LASTACK,
+	TCP_ESTATS_STATE_CLOSING,
+	TCP_ESTATS_STATE_TIMEWAIT,
+	TCP_ESTATS_STATE_DELETECB
+};
+
+enum {
+	TCP_ESTATS_TYPE_INTEGER = 0,
+	TCP_ESTATS_TYPE_INTEGER32,
+	TCP_ESTATS_TYPE_INET_ADDRESS_IPV4,
+	TCP_ESTATS_TYPE_COUNTER32,
+	TCP_ESTATS_TYPE_GAUGE32,
+	TCP_ESTATS_TYPE_UNSIGNED32,
+	TCP_ESTATS_TYPE_TIME_TICKS,
+	TCP_ESTATS_TYPE_COUNTER64,
+	TCP_ESTATS_TYPE_INET_PORT_NUMBER,
+	TCP_ESTATS_TYPE_INET_ADDRESS,
+	TCP_ESTATS_TYPE_INET_ADDRESS_IPV6,
+	TCP_ESTATS_TYPE_OCTET = 12,
+};
+
+
+/*
+ * Variables that can be read and written directly.
+ *
+ * Should contain most variables from TCP-KIS 0.1.  Commented feilds are
+ * either not implemented or have handlers and do not need struct storage.
+ */
+struct tcp_estats_directs {
+	/* Connection table */
+	u32			LocalAddressType;
+	struct { u8 data[17]; }	LocalAddress;
+	struct { u8 data[17]; }	RemAddress;
+	u16			LocalPort;
+	u16			RemPort;
+
+	/* Perf table */
+	u32		SegsOut;
+	u32		DataSegsOut;
+	u64		DataOctetsOut;
+	u32		SegsRetrans;
+	u32		OctetsRetrans;
+	u32		SegsIn;
+	u32		DataSegsIn;
+	u64		DataOctetsIn;
+	/*		ElapsedSecs */
+	/*		ElapsedMicroSecs */
+	/*		StartTimeStamp */
+	/*		CurMSS */
+	/*		PipeSize */
+	u32		MaxPipeSize;
+	/*		SmoothedRTT */
+	/*		CurRTO */
+	u32		CongSignals;
+	/*		CurCwnd */
+	/*		CurSsthresh */
+	u32		Timeouts;
+	/*		CurRwinSent */
+	u32		MaxRwinSent;
+	u32		ZeroRwinSent;
+	/*		CurRwinRcvd */
+	u32		MaxRwinRcvd;
+	u32		ZeroRwinRcvd;
+	/*		SndLimTransRwin */
+	/*		SndLimTransCwnd */
+	/*		SndLimTransSnd */
+	/*		SndLimTimeRwin */
+	/*		SndLimTimeCwnd */
+	/*		SndLimTimeSnd */
+	u32		snd_lim_trans[TCP_ESTATS_SNDLIM_NSTATES];
+	u32		snd_lim_time[TCP_ESTATS_SNDLIM_NSTATES];
+	
+	/* Path table */
+	/*		RetranThresh */
+	u32		NonRecovDAEpisodes;
+	u32		SumOctetsReordered;
+	u32		NonRecovDA;
+	u32		SampleRTT;
+	/*		RTTVar */
+	u32		MaxRTT;
+	u32		MinRTT;
+	u64		SumRTT;
+	u32		CountRTT;
+	u32		MaxRTO;
+	u32		MinRTO;
+	u8		IpTtl;
+	u8		IpTosIn;
+	/*		IpTosOut */
+	u32		PreCongSumCwnd;
+	u32		PreCongSumRTT;
+	u32		PostCongSumRTT;
+	u32		PostCongCountRTT;
+	u32		ECNsignals;
+	u32		DupAckEpisodes;
+	/*		RcvRTT */
+	u32		DupAcksOut;
+	u32		CERcvd;
+	u32		ECESent;
+	
+	/* Stack table */
+	u32		ActiveOpen;
+	/*		MSSSent */
+	/* 		MSSRcvd */
+	/*		WinScaleSent */
+	/*		WinScaleRcvd */
+	/*		TimeStamps */
+	/*		ECN */
+	/*		WillSendSACK */
+	/*		WillUseSACK */
+	/*		State */
+	/*		Nagle */
+	u32		MaxSsCwnd;
+	u32		MaxCaCwnd;
+	u32		MaxSsthresh;
+	u32		MinSsthresh;
+	/*		InRecovery */
+	u32		DupAcksIn;
+	u32		SpuriousFrDetected;
+	u32		SpuriousRtoDetected;
+	u32		SoftErrors;
+	u32		SoftErrorReason;
+	u32		SlowStart;
+	u32		CongAvoid;
+	u32		OtherReductions;
+	u32		CongOverCount;
+	u32		FastRetran;
+	u32		SubsequentTimeouts;
+	/*		CurTimeoutCount */
+	u32		AbruptTimeouts;
+	u32		SACKsRcvd;
+	u32		SACKBlocksRcvd;
+	u32		SendStall;
+	u32		DSACKDups;
+	u32		MaxMSS;
+	u32		MinMSS;
+	u32		SndInitial;
+	u32		RecInitial;
+	u32		CurRetxQueue;
+	u32		MaxRetxQueue;
+	/*		CurReasmQueue */
+	u32		MaxReasmQueue;
+
+	/* App table */
+	/*		SndUna */
+	/*		SndNxt */
+	u32		SndMax;
+	u64		ThruOctetsAcked;
+	/*		RcvNxt */
+	u64		ThruOctetsReceived;
+	/*		CurAppWQueue */
+	u32		MaxAppWQueue;
+	/*		CurAppRQueue */
+	u32		MaxAppRQueue;
+	
+	/* Tune table */
+	/*		LimCwnd */
+	/*		LimSsthresh */
+	/*		LimRwin */
+	
+	/* Extras */
+	u32		OtherReductionsCV;
+	u32		OtherReductionsCM;
+};
+
+struct tcp_estats {
+	int				estats_cid;
+
+	struct sock			*estats_sk;
+
+	atomic_t			estats_users;
+	u8				estats_dead;
+
+	struct list_head		estats_list;
+	struct list_head		estats_hash_list;
+
+	struct tcp_estats		*estats_death_next;
+
+	int				estats_limstate;
+	ktime_t				estats_limstate_ts;
+	ktime_t				estats_start_ts;
+	ktime_t				estats_current_ts;
+	struct timeval			estats_start_tv;
+
+	struct tcp_estats_directs	estats_vars;
+};
+
+
+struct tcp_estats_var;
+typedef void (*estats_rwfunc_t)(void *buf, struct tcp_estats *stats,
+                                struct tcp_estats_var *vp);
+
+/* The printed variable description should look something like this (in ASCII):
+ * varname offset type
+ * where offset is the offset into the file.
+ */
+struct tcp_estats_var {
+	char		*name;
+	u32		type;
+
+	estats_rwfunc_t	read;
+	unsigned long	read_data;	/* read handler-specific data */
+
+	estats_rwfunc_t	write;
+	unsigned long	write_data;	/* write handler-specific data */
+};
+
+extern int tcp_estats_conn_num;
+extern struct tcp_estats_var tcp_estats_var_table[];
+extern struct list_head *tcp_estats_head;
+extern rwlock_t tcp_estats_linkage_lock;
+
+/* For /proc/web100 */
+extern struct tcp_estats *tcp_estats_lookup(int cid);
+
+/* For the TCP code */
+extern int  tcp_estats_create(struct sock *sk, enum tcp_estats_addrtype t);
+extern void tcp_estats_destroy(struct sock *sk);
+extern void tcp_estats_free(struct tcp_estats *stats);
+extern void tcp_estats_establish(struct sock *sk);
+
+extern void tcp_estats_tune_sndbuf_ack(struct sock *sk);
+extern void tcp_estats_tune_sndbuf_snd(struct sock *sk);
+extern void tcp_estats_tune_rcvbuf(struct sock *sk);
+
+extern void tcp_estats_update_snd_nxt(struct tcp_sock *tp);
+extern void tcp_estats_update_acked(struct tcp_sock *tp, u32 ack);
+extern void tcp_estats_update_rtt(struct sock *sk, unsigned long rtt_sample);
+extern void tcp_estats_update_timeout(struct sock *sk);
+extern void tcp_estats_update_mss(struct tcp_sock *tp);
+extern void tcp_estats_update_rwin_rcvd(struct tcp_sock *tp);
+extern void tcp_estats_update_sndlim(struct tcp_sock *tp, int why);
+extern void tcp_estats_update_rcvd(struct tcp_sock *tp, u32 seq);
+extern void tcp_estats_update_rwin_sent(struct tcp_sock *tp);
+extern void tcp_estats_update_congestion(struct tcp_sock *tp);
+extern void tcp_estats_update_post_congestion(struct tcp_sock *tp);
+extern void tcp_estats_update_segsend(struct sock *sk, int len, int pcount,
+                                      u32 seq, u32 end_seq, int flags);
+extern void tcp_estats_update_segrecv(struct tcp_sock *tp, struct sk_buff *skb);
+extern void tcp_estats_update_finish_segrecv(struct tcp_sock *tp);
+extern void tcp_estats_update_rcvbuf(struct sock *sk, int rcvbuf);
+extern void tcp_estats_update_writeq(struct sock *sk);
+extern void tcp_estats_update_recvq(struct sock *sk);
+extern void tcp_estats_update_ofoq(struct sock *sk);
+
+extern void tcp_estats_init(void);
+extern int tcp_estats_proc_init(void);
+
+/* You may have to hold tcp_estats_linkage_lock here to prevent
+   stats from disappearing. */
+static inline void tcp_estats_use(struct tcp_estats *stats)
+{
+	atomic_inc(&stats->estats_users);
+}
+
+/* You MUST NOT hold tcp_estats_linkage_lock here. */
+static inline void tcp_estats_unuse(struct tcp_estats *stats)
+{
+	if (atomic_dec_and_test(&stats->estats_users))
+		tcp_estats_free(stats);
+}
+
+/* Length of various MIB data types. */
+static inline int tcp_estats_var_len(struct tcp_estats_var *vp)
+{
+	switch (vp->type) {
+	case TCP_ESTATS_TYPE_INET_PORT_NUMBER:
+		return 2;
+	case TCP_ESTATS_TYPE_INTEGER:
+	case TCP_ESTATS_TYPE_INTEGER32:
+	case TCP_ESTATS_TYPE_COUNTER32:
+	case TCP_ESTATS_TYPE_GAUGE32:
+	case TCP_ESTATS_TYPE_UNSIGNED32:
+	case TCP_ESTATS_TYPE_TIME_TICKS:
+		return 4;
+	case TCP_ESTATS_TYPE_COUNTER64:
+		return 8;
+	case TCP_ESTATS_TYPE_INET_ADDRESS:
+		return 17;
+	case TCP_ESTATS_TYPE_OCTET:
+		return 1;
+	}
+	
+	printk(KERN_WARNING
+	       "TCP ESTATS: Adding variable of unknown type %d.\n", vp->type);
+	return 0;
+}
+
+#else /* !CONFIG_TCP_ESTATS */
+
+#define tcp_estats_enabled	(0)
+
+#define TCP_ESTATS_VAR_INC(tp,var)	do {} while (0)
+#define TCP_ESTATS_VAR_DEC(tp,var)	do {} while (0)
+#define TCP_ESTATS_VAR_SET(tp,var,val)	do {} while (0)
+#define TCP_ESTATS_VAR_ADD(tp,var,val)	do {} while (0)
+#define TCP_ESTATS_UPDATE(tp,func)	do {} while (0)
+
+static inline void tcp_estats_init(void) { }
+static inline void tcp_estats_establish(struct sock *sk) { }
+static inline void tcp_estats_create(struct sock *sk, enum tcp_estats_addrtype t) { }
+static inline void tcp_estats_destroy(struct sock *sk) { }
+
+#endif /* CONFIG_TCP_ESTATS */
+
+#endif /* _TCP_ESTATS_H */
diff --git a/net/ipv4/Kconfig b/net/ipv4/Kconfig
index cbb505b..564d012 100644
--- a/net/ipv4/Kconfig
+++ b/net/ipv4/Kconfig
@@ -624,3 +624,61 @@ config TCP_MD5SIG
 	  on the Internet.
 
 	  If unsure, say N.
+
+config TCP_ESTATS
+	bool "TCP: Extended TCP statistics (TCP ESTATS) MIB"
+	depends on PROC_FS
+	---help---
+	  Support for the TCP extended stastics MIB
+	  (see http://www.web100.org/mib/).
+
+if TCP_ESTATS
+
+config TCP_ESTATS_MAX_CONNS
+	int "TCP: Maximum number of ESTATS conections"
+	depends on TCP_ESTATS
+	default 32768
+	---help---
+	  The maximum number of TCP connections for which we concurrently
+	  save extended statistics.  The stats consume extra memory per
+	  connection, so this helps limit the amount of total memory
+	  devoted to statistics.  It also bounds the time for user-mode
+	  tools to search for a particular connection.
+	  
+	  If this number is too small, it will not limit creation of TCP
+	  connections, but some connections will not have extended
+	  stats.
+
+config TCP_ESTATS_FPERMS
+	int "TCP: ESTATS default file permissions"
+	depends on TCP_ESTATS
+	default "384"
+	---help---
+	  This controls the default file permission bits on the ESTATS
+	  files in /proc/web100.  This value can be changed at runtime using
+	  the sysctl variable net.ipv4.tcp_eststs_fperms.  Unless all users on
+	  the system are trusted, it is safest to limit both readability
+	  and writability to trusted users.
+
+	  Due to limitations of the kernel config scripts, this is a decimal
+	  value rather than octal.  Some common values:
+
+	    384 = 0600 = rw-------
+	    416 = 0640 = rw-r-----
+	    432 = 0660 = rw-rw----
+	    436 = 0664 = rw-rw-r--
+	    438 = 0666 = rw-rw-rw-
+
+config TCP_ESTATS_GID
+	int "TCP: ESTATS default gid"
+	depends on TCP_ESTATS
+	default "0"
+	---help---
+	  This will be the default group of the ESTATS files in /proc/web100.
+	  It may be useful to create a "tcp_estats" group on your system, and set
+	  CONFIG_TCP_ESTATS_FPERMS (above) with special group permissions.  This
+	  value can be changed at runtime using the sysctl variable
+	  net.ipv4.tcp_estats_gid.
+
+endif
+
diff --git a/net/ipv4/Makefile b/net/ipv4/Makefile
index f2dc69c..7497fe5 100644
--- a/net/ipv4/Makefile
+++ b/net/ipv4/Makefile
@@ -31,6 +31,7 @@ obj-$(CONFIG_INET_TUNNEL) += tunnel4.o
 obj-$(CONFIG_INET_XFRM_MODE_TRANSPORT) += xfrm4_mode_transport.o
 obj-$(CONFIG_INET_XFRM_MODE_TUNNEL) += xfrm4_mode_tunnel.o
 obj-$(CONFIG_IP_PNP) += ipconfig.o
+obj-$(CONFIG_TCP_ESTATS) += tcp_estats.o tcp_estats_proc.o
 obj-$(CONFIG_NETFILTER)	+= netfilter.o netfilter/
 obj-$(CONFIG_INET_DIAG) += inet_diag.o 
 obj-$(CONFIG_INET_TCP_DIAG) += tcp_diag.o
diff --git a/net/ipv4/sysctl_net_ipv4.c b/net/ipv4/sysctl_net_ipv4.c
index 57d0752..d31cf29 100644
--- a/net/ipv4/sysctl_net_ipv4.c
+++ b/net/ipv4/sysctl_net_ipv4.c
@@ -675,6 +675,22 @@ static struct ctl_table ipv4_table[] = {
 		.proc_handler	= proc_dointvec_minmax,
 		.extra1		= &zero
 	},
+#ifdef CONFIG_TCP_ESTATS
+	{
+		.procname	= "tcp_estats_fperms",
+		.data		= &sysctl_tcp_estats_fperms,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler   = &tcp_estats_proc_dointvec_update,
+  	},
+	{
+		.procname	= "tcp_estats_gid",
+		.data		= &sysctl_tcp_estats_gid,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= &tcp_estats_proc_dointvec_update,
+	},
+#endif
 	{ }
 };
 
diff --git a/net/ipv4/tcp.c b/net/ipv4/tcp.c
index 46febca..3417147 100644
--- a/net/ipv4/tcp.c
+++ b/net/ipv4/tcp.c
@@ -290,6 +290,11 @@ EXPORT_SYMBOL(sysctl_tcp_mem);
 EXPORT_SYMBOL(sysctl_tcp_rmem);
 EXPORT_SYMBOL(sysctl_tcp_wmem);
 
+#ifdef CONFIG_TCP_ESTATS
+int sysctl_tcp_estats_fperms = CONFIG_TCP_ESTATS_FPERMS;
+int sysctl_tcp_estats_gid = CONFIG_TCP_ESTATS_GID;
+#endif
+
 atomic_long_t tcp_memory_allocated;	/* Current allocated memory. */
 EXPORT_SYMBOL(tcp_memory_allocated);
 
@@ -850,8 +855,10 @@ new_segment:
 wait_for_sndbuf:
 		set_bit(SOCK_NOSPACE, &sk->sk_socket->flags);
 wait_for_memory:
-		if (copied)
+		if (copied) {
 			tcp_push(sk, flags & ~MSG_MORE, mss_now, TCP_NAGLE_PUSH);
+			TCP_ESTATS_UPDATE(tp, tcp_estats_update_writeq(sk));
+		}
 
 		if ((err = sk_stream_wait_memory(sk, &timeo)) != 0)
 			goto do_error;
@@ -1098,8 +1105,10 @@ new_segment:
 wait_for_sndbuf:
 			set_bit(SOCK_NOSPACE, &sk->sk_socket->flags);
 wait_for_memory:
-			if (copied)
+			if (copied) {
 				tcp_push(sk, flags & ~MSG_MORE, mss_now, TCP_NAGLE_PUSH);
+				TCP_ESTATS_UPDATE(tp, tcp_estats_update_writeq(sk));
+			}
 
 			if ((err = sk_stream_wait_memory(sk, &timeo)) != 0)
 				goto do_error;
@@ -1497,6 +1506,8 @@ int tcp_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
 			     *seq, TCP_SKB_CB(skb)->seq, tp->rcv_nxt, flags);
 		}
 
+		TCP_ESTATS_UPDATE(tp, tcp_estats_update_recvq(sk));
+
 		/* Well, if we have backlog, try to process it now yet. */
 
 		if (copied >= target && !sk->sk_backlog.tail)
@@ -3300,6 +3311,7 @@ void __init tcp_init(void)
 	       tcp_hashinfo.ehash_mask + 1, tcp_hashinfo.bhash_size);
 
 	tcp_register_congestion_control(&tcp_reno);
+	tcp_estats_init();
 
 	memset(&tcp_secret_one.secrets[0], 0, sizeof(tcp_secret_one.secrets));
 	memset(&tcp_secret_two.secrets[0], 0, sizeof(tcp_secret_two.secrets));
diff --git a/net/ipv4/tcp_cong.c b/net/ipv4/tcp_cong.c
index 850c737..a425f91 100644
--- a/net/ipv4/tcp_cong.c
+++ b/net/ipv4/tcp_cong.c
@@ -366,11 +366,14 @@ void tcp_reno_cong_avoid(struct sock *sk, u32 ack, u32 in_flight)
 		return;
 
 	/* In "safe" area, increase. */
-	if (tp->snd_cwnd <= tp->snd_ssthresh)
+	if (tp->snd_cwnd <= tp->snd_ssthresh) {
 		tcp_slow_start(tp);
+		TCP_ESTATS_VAR_INC(tp, SlowStart);
+		return;
+	}
 
 	/* In dangerous area, increase slowly. */
-	else if (sysctl_tcp_abc) {
+	if (sysctl_tcp_abc) {
 		/* RFC3465: Appropriate Byte Count
 		 * increase once for each full cwnd acked
 		 */
@@ -382,6 +385,7 @@ void tcp_reno_cong_avoid(struct sock *sk, u32 ack, u32 in_flight)
 	} else {
 		tcp_cong_avoid_ai(tp, tp->snd_cwnd);
 	}
+	TCP_ESTATS_VAR_INC(tp, CongAvoid);
 }
 EXPORT_SYMBOL_GPL(tcp_reno_cong_avoid);
 
diff --git a/net/ipv4/tcp_estats.c b/net/ipv4/tcp_estats.c
new file mode 100644
index 0000000..576a0b3
--- /dev/null
+++ b/net/ipv4/tcp_estats.c
@@ -0,0 +1,1177 @@
+/*
+ * net/ipv4/tcp_estats.c
+ *
+ * Implementation of TCP ESTATS MIB (RFC 4898)
+ *
+ * Authors:
+ *   John Heffner <jheffner@psc.edu>
+ *   Matt Mathis <mathis@psc.edu>
+ *   Jeff Semke <semke@psc.edu>
+ *
+ * The Web10Gig project.  See http://www.web10gig.org
+ *
+ * Copyright Â© 2011, Pittsburgh Supercomputing Center (PSC).
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ *
+ */
+
+#include <linux/types.h>
+#include <linux/socket.h>
+#include <linux/string.h>
+#include <linux/proc_fs.h>
+#include <linux/bootmem.h>
+#include <linux/list.h>
+#include <net/tcp_estats.h>
+#include <net/tcp.h>
+#include <asm/atomic.h>
+#include <asm/byteorder.h>
+
+#define ESTATS_INF32	0xffffffff
+
+#define ESTATS_DEATH_SLOTS	8
+#define ESTATS_PERSIST_TIME	60
+
+int tcp_estats_enabled __read_mostly = 0;
+
+static void death_cleanup(unsigned long dummy);
+
+/* Global stats reader-writer lock */
+DEFINE_RWLOCK(tcp_estats_linkage_lock);
+
+/* Data structures for tying together stats */
+static int tcp_estats_next_cid;
+static int tcp_estats_htsize;
+int tcp_estats_conn_num;
+struct list_head *tcp_estats_ht;
+struct list_head *tcp_estats_head;
+
+static struct tcp_estats *death_slots[ESTATS_DEATH_SLOTS];
+static int cur_death_slot;
+static DEFINE_SPINLOCK(death_lock);
+static struct timer_list stats_persist_timer = TIMER_INITIALIZER(death_cleanup, 0, 0);
+static int ndeaths;
+
+extern struct proc_dir_entry *proc_tcp_estats_dir;
+
+/*
+ * Structural maintainance
+ */
+
+static inline int tcp_estats_hash(int cid)
+{
+	return cid % tcp_estats_htsize;
+}
+
+static inline int tcp_estats_hashstep(int cid)
+{
+	return ((cid + tcp_estats_htsize) & 0x7fffffff);
+}
+
+struct tcp_estats *tcp_estats_lookup(int cid)
+{
+	struct list_head *p, *bucket_head;
+	struct tcp_estats *stats;
+
+	bucket_head = &tcp_estats_ht[tcp_estats_hash(cid)];
+	list_for_each(p, bucket_head) {
+		stats = list_entry(p, struct tcp_estats, estats_hash_list);
+		if (stats->estats_cid == cid)
+			return stats;
+	}
+	
+	return NULL;
+}
+
+static int get_next_cid(void)
+{
+	u32 i;
+
+	if (tcp_estats_conn_num >= CONFIG_TCP_ESTATS_MAX_CONNS)
+		return -1;
+
+	i = (u32)tcp_estats_next_cid;
+	while (tcp_estats_lookup(i)) {
+		if ((i = tcp_estats_hashstep(i)) == tcp_estats_next_cid) {
+			tcp_estats_next_cid = (tcp_estats_next_cid + 1) & 0x7fffffff;
+			i = (u32)tcp_estats_next_cid;
+		}
+	}
+	tcp_estats_next_cid = (tcp_estats_next_cid + 1) & 0x7fffffff;
+
+	return i;
+}
+
+static void stats_link(struct tcp_estats *stats)
+{
+	write_lock_bh(&tcp_estats_linkage_lock);
+
+	if ((stats->estats_cid = get_next_cid()) < 0) {
+		write_unlock_bh(&tcp_estats_linkage_lock);
+		return;
+	}
+
+	list_add(&stats->estats_hash_list,
+		&tcp_estats_ht[tcp_estats_hash(stats->estats_cid)]);
+	list_add(&stats->estats_list, tcp_estats_head);
+
+	tcp_estats_conn_num++;
+	proc_tcp_estats_dir->nlink = tcp_estats_conn_num + 2;
+
+	write_unlock_bh(&tcp_estats_linkage_lock);
+}
+
+static void stats_unlink(struct tcp_estats *stats)
+{
+	write_lock_bh(&tcp_estats_linkage_lock);
+
+	list_del(&stats->estats_hash_list);
+	list_del(&stats->estats_list);
+
+	tcp_estats_conn_num--;
+	proc_tcp_estats_dir->nlink = tcp_estats_conn_num + 2;
+
+	write_unlock_bh(&tcp_estats_linkage_lock);
+}
+
+static void stats_persist(struct tcp_estats *stats)
+{
+	spin_lock_bh(&death_lock);
+
+	stats->estats_death_next = death_slots[cur_death_slot];
+	death_slots[cur_death_slot] = stats;
+	if (ndeaths <= 0)
+		mod_timer(&stats_persist_timer,
+			  jiffies +
+			  ESTATS_PERSIST_TIME * HZ / ESTATS_DEATH_SLOTS);
+	ndeaths++;
+
+	spin_unlock_bh(&death_lock);
+}
+
+static void death_cleanup(unsigned long dummy)
+{
+	struct tcp_estats *stats, *next;
+
+	spin_lock_bh(&death_lock);
+
+	cur_death_slot = (cur_death_slot + 1) % ESTATS_DEATH_SLOTS;
+	stats = death_slots[cur_death_slot];
+	while (stats) {
+		stats->estats_dead = 1;
+		ndeaths--;
+		next = stats->estats_death_next;
+		tcp_estats_unuse(stats);
+		stats = next;
+	}
+	death_slots[cur_death_slot] = NULL;
+
+	if (ndeaths > 0)
+		mod_timer(&stats_persist_timer,
+			  jiffies +
+			  ESTATS_PERSIST_TIME * HZ / ESTATS_DEATH_SLOTS);
+
+	spin_unlock_bh(&death_lock);
+}
+
+/* Called whenever a TCP/IPv4 sock is created.
+ * net/ipv4/tcp_ipv4.c: tcp_v4_syn_recv_sock,
+ *			tcp_v4_init_sock
+ * Allocates a stats structure and initializes values.
+ */
+int tcp_estats_create(struct sock *sk, enum tcp_estats_addrtype addrtype)
+{
+	struct tcp_estats *stats;
+	struct tcp_estats_directs *vars;
+	struct tcp_sock *tp = tcp_sk(sk);
+
+	if (!tcp_estats_enabled) {
+		stats = NULL;
+		return -1;
+	}
+
+	stats = kzalloc(sizeof(struct tcp_estats), gfp_any());
+	if (!stats)
+		return -ENOMEM;
+
+	tp->tcp_stats = stats;
+	vars = &stats->estats_vars;
+
+	stats->estats_cid = -1;
+	stats->estats_vars.LocalAddressType = addrtype;
+
+	sock_hold(sk);
+	stats->estats_sk = sk;
+	atomic_set(&stats->estats_users, 0);
+
+	stats->estats_limstate = TCP_ESTATS_SNDLIM_STARTUP;
+	stats->estats_start_ts = stats->estats_limstate_ts =
+	    stats->estats_current_ts = ktime_get();
+	do_gettimeofday(&stats->estats_start_tv);
+
+	vars->ActiveOpen = !in_interrupt();
+
+	vars->SndMax = tp->snd_nxt;
+	vars->SndInitial = tp->snd_nxt;
+
+	vars->MinRTT = vars->MinRTO = vars->MinMSS = vars->MinSsthresh =
+	    ESTATS_INF32;
+
+	tcp_estats_use(stats);
+
+	return 0;
+}
+
+void tcp_estats_destroy(struct sock *sk)
+{
+	struct tcp_estats *stats = tcp_sk(sk)->tcp_stats;
+
+	if (stats == NULL)
+		return;
+
+	/* Attribute final sndlim time. */
+	tcp_estats_update_sndlim(tcp_sk(stats->estats_sk),
+				 stats->estats_limstate);
+
+	if (stats->estats_cid >= 0)
+		stats_persist(stats);
+	else
+		tcp_estats_unuse(stats);
+}
+
+/* Do not call directly.  Called from tcp_estats_unuse(). */
+void tcp_estats_free(struct tcp_estats *stats)
+{
+	if (stats->estats_cid >= 0) {
+		stats_unlink(stats);
+	}
+	sock_put(stats->estats_sk);
+	kfree(stats);
+}
+
+/* Called when a connection enters the ESTABLISHED state, and has all its
+ * state initialized.
+ * net/ipv4/tcp_input.c: tcp_rcv_state_process,
+ *			 tcp_rcv_synsent_state_process
+ * Here we link the statistics structure in so it is visible in the /proc
+ * fs, and do some final init.
+ */
+void tcp_estats_establish(struct sock *sk)
+{
+	struct inet_sock *inet = inet_sk(sk);
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct tcp_estats *stats = tp->tcp_stats;
+	struct tcp_estats_directs *vars = &stats->estats_vars;
+
+	if (stats == NULL)
+		return;
+
+	/* Let's set these here, since they can't change once the
+	 * connection is established.
+	 */
+	vars->LocalPort = inet->inet_num;
+	vars->RemPort = ntohs(inet->inet_dport);
+
+	if (vars->LocalAddressType == TCP_ESTATS_ADDRTYPE_IPV4) {
+		memcpy(&vars->LocalAddress, &inet->inet_rcv_saddr, 4);
+		memcpy(&vars->RemAddress, &inet->inet_daddr, 4);
+	}
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+	else if (vars->LocalAddressType == TCP_ESTATS_ADDRTYPE_IPV6) {
+		memcpy(&vars->LocalAddress, &(inet6_sk(sk)->saddr), 16);
+		memcpy(&vars->RemAddress, &(inet6_sk(sk)->daddr), 16);
+	}
+#endif
+	else {
+		printk(KERN_ERR "TCP ESTATS: LocalAddressType not valid.\n");
+	}
+	((char *)&vars->LocalAddress)[16] = ((char *)&vars->RemAddress)[16] =
+	    vars->LocalAddressType;
+
+	tcp_estats_update_finish_segrecv(tp);
+	tcp_estats_update_rwin_rcvd(tp);
+	tcp_estats_update_rwin_sent(tp);
+
+	vars->RecInitial = tp->rcv_nxt;
+
+	stats_link(stats);
+
+	tcp_estats_update_sndlim(tp, TCP_ESTATS_SNDLIM_SENDER);
+}
+
+/*
+ * Statistics update functions
+ */
+
+void tcp_estats_update_snd_nxt(struct tcp_sock *tp)
+{
+	struct tcp_estats *stats = tp->tcp_stats;
+
+	if (after(tp->snd_nxt, stats->estats_vars.SndMax))
+		stats->estats_vars.SndMax = tp->snd_nxt;
+}
+
+void tcp_estats_update_acked(struct tcp_sock *tp, u32 ack)
+{
+	struct tcp_estats *stats = tp->tcp_stats;
+
+	stats->estats_vars.ThruOctetsAcked += ack - tp->snd_una;
+}
+
+void tcp_estats_update_rtt(struct sock *sk, unsigned long rtt_sample)
+{
+	struct tcp_estats *stats = tcp_sk(sk)->tcp_stats;
+	unsigned long rtt_sample_msec = rtt_sample * 1000 / HZ;
+	u32 rto;
+
+	stats->estats_vars.SampleRTT = rtt_sample_msec;
+
+	if (rtt_sample_msec > stats->estats_vars.MaxRTT)
+		stats->estats_vars.MaxRTT = rtt_sample_msec;
+	if (rtt_sample_msec < stats->estats_vars.MinRTT)
+		stats->estats_vars.MinRTT = rtt_sample_msec;
+
+	stats->estats_vars.CountRTT++;
+	stats->estats_vars.SumRTT += rtt_sample_msec;
+
+	rto = inet_csk(sk)->icsk_rto * 1000 / HZ;
+	if (rto > stats->estats_vars.MaxRTO)
+		stats->estats_vars.MaxRTO = rto;
+	if (rto < stats->estats_vars.MinRTO)
+		stats->estats_vars.MinRTO = rto;
+}
+
+void tcp_estats_update_timeout(struct sock *sk)
+{
+	struct tcp_estats *stats = tcp_sk(sk)->tcp_stats;
+
+	if (inet_csk(sk)->icsk_backoff)
+		stats->estats_vars.SubsequentTimeouts++;
+	else
+		stats->estats_vars.Timeouts++;
+	if (inet_csk(sk)->icsk_ca_state == TCP_CA_Open)
+		stats->estats_vars.AbruptTimeouts++;
+}
+
+void tcp_estats_update_mss(struct tcp_sock *tp)
+{
+	struct tcp_estats *stats = tp->tcp_stats;
+	int mss = tp->mss_cache;
+
+	if (mss > stats->estats_vars.MaxMSS)
+		stats->estats_vars.MaxMSS = mss;
+	if (mss < stats->estats_vars.MinMSS)
+		stats->estats_vars.MinMSS = mss;
+}
+
+void tcp_estats_update_finish_segrecv(struct tcp_sock *tp)
+{
+	struct tcp_estats *stats = tp->tcp_stats;
+	struct tcp_estats_directs *vars = &stats->estats_vars;
+	u32 mss = tp->mss_cache;
+	u32 cwnd;
+	u32 ssthresh;
+	u32 pipe_size;
+
+	stats->estats_current_ts = ktime_get();
+
+	cwnd = tp->snd_cwnd * mss;
+	if (tp->snd_cwnd <= tp->snd_ssthresh) {
+		if (cwnd > vars->MaxSsCwnd)
+			vars->MaxSsCwnd = cwnd;
+	} else {
+		if (cwnd > vars->MaxCaCwnd)
+			vars->MaxCaCwnd = cwnd;
+	}
+
+	pipe_size = tcp_packets_in_flight(tp) * mss;
+	if (pipe_size > vars->MaxPipeSize)
+		vars->MaxPipeSize = pipe_size;
+
+	/* Discard initiail ssthresh set at infinity. */
+	if (tp->snd_ssthresh >= 0x7ffffff) {
+		return;
+	}
+	ssthresh = tp->snd_ssthresh * tp->mss_cache;
+	if (ssthresh > vars->MaxSsthresh)
+		vars->MaxSsthresh = ssthresh;
+	if (ssthresh < vars->MinSsthresh)
+		vars->MinSsthresh = ssthresh;
+}
+
+void tcp_estats_update_rwin_rcvd(struct tcp_sock *tp)
+{
+	struct tcp_estats *stats = tp->tcp_stats;
+	u32 win = tp->snd_wnd;
+
+	if (win > stats->estats_vars.MaxRwinRcvd)
+		stats->estats_vars.MaxRwinRcvd = win;
+	if (win == 0)
+		stats->estats_vars.ZeroRwinRcvd++;
+}
+
+void tcp_estats_update_rwin_sent(struct tcp_sock *tp)
+{
+	struct tcp_estats *stats = tp->tcp_stats;
+	u32 win = tp->rcv_wnd;
+
+	if (win > stats->estats_vars.MaxRwinSent)
+		stats->estats_vars.MaxRwinSent = win;
+	if (win == 0)
+		stats->estats_vars.ZeroRwinRcvd++;
+}
+
+void tcp_estats_update_sndlim(struct tcp_sock *tp, int why)
+{
+	struct tcp_estats *stats = tp->tcp_stats;
+	ktime_t now;
+
+	if (why < 0) {
+		printk(KERN_ERR "tcp_estats_update_sndlim: BUG: why < 0\n");
+		return;
+	}
+
+	now = ktime_get();
+	stats->estats_vars.snd_lim_time[stats->estats_limstate]
+	    += ktime_to_ns(ktime_sub(now, stats->estats_limstate_ts));
+
+	stats->estats_limstate_ts = now;
+	if (stats->estats_limstate != why) {
+		stats->estats_limstate = why;
+		stats->estats_vars.snd_lim_trans[why]++;
+	}
+}
+
+void tcp_estats_update_congestion(struct tcp_sock *tp)
+{
+	struct tcp_estats *stats = tp->tcp_stats;
+
+	stats->estats_vars.CongSignals++;
+	stats->estats_vars.PreCongSumCwnd += tp->snd_cwnd * tp->mss_cache;
+	stats->estats_vars.PreCongSumRTT += stats->estats_vars.SampleRTT;
+}
+
+void tcp_estats_update_post_congestion(struct tcp_sock *tp)
+{
+	struct tcp_estats *stats = tp->tcp_stats;
+	
+	stats->estats_vars.PostCongCountRTT++;
+	stats->estats_vars.PostCongSumRTT += stats->estats_vars.SampleRTT;
+}
+
+void tcp_estats_update_segsend(struct sock *sk, int len, int pcount,
+			       u32 seq, u32 end_seq, int flags)
+{
+	struct tcp_estats *stats = tcp_sk(sk)->tcp_stats;
+
+	stats->estats_current_ts = ktime_get();
+
+	/* We know we're sending a segment. */
+	stats->estats_vars.SegsOut += pcount;
+
+	/* A pure ACK contains no data; everything else is data. */
+	if (len > 0) {
+		stats->estats_vars.DataSegsOut += pcount;
+		stats->estats_vars.DataOctetsOut += len;
+	}
+
+	/* Check for retransmission. */
+	if (flags & TCPHDR_SYN) {
+		if (inet_csk(sk)->icsk_retransmits)
+			stats->estats_vars.SegsRetrans++;
+	} else if (before(seq, stats->estats_vars.SndMax)) {
+		stats->estats_vars.SegsRetrans += pcount;
+		stats->estats_vars.OctetsRetrans += end_seq - seq;
+	}
+}
+
+void tcp_estats_update_segrecv(struct tcp_sock *tp, struct sk_buff *skb)
+{
+	struct tcp_estats_directs *vars = &tp->tcp_stats->estats_vars;
+	struct tcphdr *th = tcp_hdr(skb);
+	struct iphdr *iph = ip_hdr(skb);
+
+	vars->SegsIn++;
+	if (skb->len == th->doff * 4) {
+		if (TCP_SKB_CB(skb)->ack_seq == tp->snd_una)
+			vars->DupAcksIn++;
+	} else {
+		vars->DataSegsIn++;
+		vars->DataOctetsIn += skb->len - th->doff * 4;
+	}
+
+	vars->IpTtl = iph->ttl;
+	vars->IpTosIn = iph->tos;
+}
+
+void tcp_estats_update_rcvd(struct tcp_sock *tp, u32 seq)
+{
+	struct tcp_estats *stats = tp->tcp_stats;
+
+	stats->estats_vars.ThruOctetsReceived += seq - tp->rcv_nxt;
+}
+
+void tcp_estats_update_writeq(struct sock *sk)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct tcp_estats_directs *vars = &tp->tcp_stats->estats_vars;
+	int len = tp->write_seq - vars->SndMax;
+
+	if (len > vars->MaxAppWQueue)
+		vars->MaxAppWQueue = len;
+}
+
+static inline u32 ofo_qlen(struct tcp_sock *tp)
+{
+	if (!skb_peek(&tp->out_of_order_queue))
+		return 0;
+	else
+		return TCP_SKB_CB(tp->out_of_order_queue.prev)->end_seq -
+		    TCP_SKB_CB(tp->out_of_order_queue.next)->seq;
+}
+
+void tcp_estats_update_recvq(struct sock *sk)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct tcp_estats_directs *vars = &tp->tcp_stats->estats_vars;
+	u32 len1 = tp->rcv_nxt - tp->copied_seq;
+	u32 len2 = ofo_qlen(tp);
+
+	if (vars->MaxAppRQueue < len1)
+		vars->MaxAppRQueue = len1;
+
+	if (vars->MaxReasmQueue < len2)
+		vars->MaxReasmQueue = len2;
+}
+
+/*
+ * Read/write functions
+ */
+/* A read handler for reading directly from the stats structure */
+static void read_stats(void *buf, struct tcp_estats *stats,
+		       struct tcp_estats_var *vp)
+{
+	memcpy(buf, (char *)stats + vp->read_data, tcp_estats_var_len(vp));
+}
+
+static void read_LimCwnd(void *buf, struct tcp_estats *stats,
+			 struct tcp_estats_var *vp)
+{
+	struct tcp_sock *tp = tcp_sk(stats->estats_sk);
+	u32 tmp = (u32) (tp->snd_cwnd_clamp * tp->mss_cache);
+
+	memcpy(buf, &tmp, 4);
+}
+
+static void write_LimCwnd(void *buf, struct tcp_estats *stats,
+			  struct tcp_estats_var *vp)
+{
+	struct tcp_sock *tp = tcp_sk(stats->estats_sk);
+
+	tp->snd_cwnd_clamp = min(*(u32 *) buf / tp->mss_cache, 65535U);
+}
+
+static void read_LimSsthresh(void *buf, struct tcp_estats *stats,
+			     struct tcp_estats_var *vp)
+{
+	u32 tmp = (u32) sysctl_tcp_max_ssthresh;
+
+	if (tmp == 0)
+		tmp = 0x7fffffff;
+	memcpy(buf, &sysctl_tcp_max_ssthresh, 4);
+}
+
+static void write_LimRwin(void *buf, struct tcp_estats *stats,
+			  struct tcp_estats_var *vp)
+{
+	u32 val;
+	struct tcp_sock *tp = tcp_sk(stats->estats_sk);
+
+	memcpy(&val, buf, 4);
+	tp->window_clamp = min(val, 65535U << tp->rx_opt.rcv_wscale);
+}
+
+static void write_Sndbuf(void *buf, struct tcp_estats *stats,
+			 struct tcp_estats_var *vp)
+{
+	int val;
+	struct sock *sk = stats->estats_sk;
+
+	memcpy(&val, buf, sizeof(int));
+
+	sk->sk_userlocks |= SOCK_SNDBUF_LOCK;
+	sk->sk_sndbuf =
+	    max_t(int, SOCK_MIN_SNDBUF, min_t(int, sysctl_wmem_max, val));
+	sk->sk_write_space(sk);
+}
+
+static void write_Rcvbuf(void *buf, struct tcp_estats *stats,
+			 struct tcp_estats_var *vp)
+{
+	int val;
+	struct sock *sk = stats->estats_sk;
+
+	memcpy(&val, buf, sizeof(int));
+
+	sk->sk_userlocks |= SOCK_RCVBUF_LOCK;
+	sk->sk_rcvbuf =
+	    max_t(int, SOCK_MIN_RCVBUF, min_t(int, sysctl_rmem_max, val));
+}
+
+static void read_State(void *buf, struct tcp_estats *stats,
+		       struct tcp_estats_var *vp)
+{
+	/* A mapping from Linux to MIB state. */
+	static char state_map[] = { 0, TCP_ESTATS_STATE_ESTABLISHED,
+				    TCP_ESTATS_STATE_SYNSENT,
+				    TCP_ESTATS_STATE_SYNRECEIVED,
+				    TCP_ESTATS_STATE_FINWAIT1,
+				    TCP_ESTATS_STATE_FINWAIT2,
+				    TCP_ESTATS_STATE_TIMEWAIT,
+				    TCP_ESTATS_STATE_CLOSED,
+				    TCP_ESTATS_STATE_CLOSEWAIT,
+				    TCP_ESTATS_STATE_LASTACK,
+				    TCP_ESTATS_STATE_LISTEN,
+				    TCP_ESTATS_STATE_CLOSING };
+	s32 val = state_map[stats->estats_sk->sk_state];
+	
+	memcpy(&val, buf, 4);
+}
+
+static void write_State(void *buf, struct tcp_estats *stats,
+			struct tcp_estats_var *vp)
+{
+	int val;
+	struct sock *sk = stats->estats_sk;
+
+	memcpy(&val, buf, sizeof(int));
+	if (val != 12)		/* deleteTCB, RFC 2012 */
+		return;
+	sk->sk_prot->disconnect(sk, 0);
+}
+
+/* A read handler for reading directly from the sk for 32-bit types only! */
+static void read_sk32(void *buf, struct tcp_estats *stats,
+		      struct tcp_estats_var *vp)
+{
+	memcpy(buf, (char *)(stats->estats_sk) + vp->read_data, 4);
+}
+
+static void write_LimMSS(void *buf, struct tcp_estats *stats,
+			 struct tcp_estats_var *vp)
+{
+	struct tcp_sock *tp = tcp_sk(stats->estats_sk);
+	u32 val = *(u32 *) buf;
+
+	if (val >= (1 << 16))
+		val = (1 << 16) - 1;
+	if (val < 1)
+		val = 1;
+
+	tp->rx_opt.mss_clamp =
+	    min_t(u16, val, min(tp->rx_opt.rec_mss, tp->rx_opt.user_mss));
+	if (tp->mss_cache > tp->rx_opt.mss_clamp)
+		tp->mss_cache = tp->rx_opt.mss_clamp;
+
+	tcp_estats_update_mss(tp);
+}
+
+static void read_ElapsedSecs(void *buf, struct tcp_estats *stats,
+			     struct tcp_estats_var *vp)
+{
+	ktime_t elapsed = ktime_sub(stats->estats_current_ts,
+				    stats->estats_start_ts);
+	u32 secs = ktime_to_timeval(elapsed).tv_sec;
+
+	memcpy(buf, &secs, 4);
+}
+
+static void read_ElapsedMicroSecs(void *buf, struct tcp_estats *stats,
+				  struct tcp_estats_var *vp)
+{
+	ktime_t elapsed = ktime_sub(stats->estats_current_ts,
+				    stats->estats_start_ts);
+	u32 usecs = ktime_to_timeval(elapsed).tv_usec;
+
+	memcpy(buf, &usecs, 4);
+}
+
+static void read_StartTimeSecs(void *buf, struct tcp_estats *stats,
+			       struct tcp_estats_var *vp)
+{
+	u32 secs = (u32) stats->estats_start_tv.tv_sec;
+
+	memcpy(buf, &secs, 4);
+}
+
+static void read_StartTimeMicroSecs(void *buf, struct tcp_estats *stats,
+				    struct tcp_estats_var *vp)
+{
+	u32 usecs = (u32) stats->estats_start_tv.tv_usec;
+
+	memcpy(buf, &usecs, 4);
+}
+
+static void read_RetranThresh(void *buf, struct tcp_estats *stats,
+			      struct tcp_estats_var *vp)
+{
+	struct tcp_sock *tp = tcp_sk(stats->estats_sk);
+	u32 val;
+
+	val = tp->reordering;
+	memcpy(buf, &val, 4);
+}
+
+static void read_PipeSize(void *buf, struct tcp_estats *stats,
+			  struct tcp_estats_var *vp)
+{
+	struct tcp_sock *tp = tcp_sk(stats->estats_sk);
+	u32 val;
+
+	val = tcp_packets_in_flight(tp) * tp->mss_cache;
+	memcpy(buf, &val, 4);
+}
+
+static void read_SmoothedRTT(void *buf, struct tcp_estats *stats,
+			     struct tcp_estats_var *vp)
+{
+	struct tcp_sock *tp = tcp_sk(stats->estats_sk);
+	u32 val;
+	
+	val = (tp->srtt >> 3) * 1000 / HZ;
+	memcpy(buf, &val, 4);
+}
+
+static void read_CurRTO(void *buf, struct tcp_estats *stats,
+			struct tcp_estats_var *vp)
+{
+	struct inet_connection_sock *icsk = inet_csk(stats->estats_sk);
+	u32 val;
+
+	val = icsk->icsk_rto * 1000 / HZ;
+	memcpy(buf, &val, 4);
+}
+
+static void read_RTTVar(void *buf, struct tcp_estats *stats,
+			struct tcp_estats_var *vp)
+{
+	struct tcp_sock *tp = tcp_sk(stats->estats_sk);
+	u32 val;
+
+	val = (tp->rttvar >> 2) * 1000 / HZ;
+	memcpy(buf, &val, 4);
+}
+
+static void read_RcvRTT(void *buf, struct tcp_estats *stats,
+			struct tcp_estats_var *vp)
+{
+	struct tcp_sock *tp = tcp_sk(stats->estats_sk);
+	u32 val;
+	
+	val = ((1000000*tp->rcv_rtt_est.rtt)/HZ)>>3;
+	memcpy(buf, &val, 4);
+}
+
+static void read_CurCwnd(void *buf, struct tcp_estats *stats,
+			 struct tcp_estats_var *vp)
+{
+	struct tcp_sock *tp = tcp_sk(stats->estats_sk);
+	u32 val;
+
+	val = tp->snd_cwnd * tp->mss_cache;
+	memcpy(buf, &val, 4);
+}
+
+static void read_CurSsthresh(void *buf, struct tcp_estats *stats,
+			     struct tcp_estats_var *vp)
+{
+	struct tcp_sock *tp = tcp_sk(stats->estats_sk);
+	u32 val;
+
+	val = tp->snd_ssthresh == 0x7fffffff ?
+	      tp->snd_ssthresh * tp->mss_cache : 0xffffffff;
+	memcpy(buf, &val, 4);
+}
+
+/* Note: this value returned is technically incorrect between a
+ * setsockopt of IP_TOS, and when the next segment is sent. */
+static void read_IpTosOut(void *buf, struct tcp_estats *stats,
+			  struct tcp_estats_var *vp)
+{
+	struct inet_sock *inet = inet_sk(stats->estats_sk);
+
+	*(char *)buf = inet->tos;
+}
+
+static void read_InRecovery(void *buf, struct tcp_estats *stats,
+			    struct tcp_estats_var *vp)
+{
+	struct inet_connection_sock *icsk = inet_csk(stats->estats_sk);
+	s32 val;
+
+	val = icsk->icsk_ca_state > TCP_CA_CWR ? 1 : 2;
+	memcpy(buf, &val, 4);
+}
+
+static void read_CurTimeoutCount(void *buf, struct tcp_estats *stats,
+				 struct tcp_estats_var *vp)
+{
+	struct inet_connection_sock *icsk = inet_csk(stats->estats_sk);
+	u32 val;
+	
+	val = icsk->icsk_retransmits;
+	memcpy(buf, &val, 4);
+}
+
+/* Note: all these (Nagle, SACK, ECN, TimeStamps) are incorrect
+ * if the sysctl values are changed during the connection. */
+static void read_Nagle(void *buf, struct tcp_estats *stats,
+		       struct tcp_estats_var *vp)
+{
+	struct tcp_sock *tp = tcp_sk(stats->estats_sk);
+	s32 val;
+
+	val = tp->nonagle ? 2 : 1;
+	memcpy(buf, &val, 4);
+}
+
+static void read_WillSendSACK(void *buf, struct tcp_estats *stats,
+			      struct tcp_estats_var *vp)
+{
+	struct tcp_sock *tp = tcp_sk(stats->estats_sk);
+	s32 val;
+
+	if (tp->rx_opt.sack_ok)
+		val = 1;
+	else
+		val = sysctl_tcp_sack ? 3 : 2;
+	memcpy(buf, &val, 4);
+}
+
+#define read_WillUseSACK	read_WillSendSACK
+
+static void read_ECN(void *buf, struct tcp_estats *stats,
+		     struct tcp_estats_var *vp)
+{
+	struct tcp_sock *tp = tcp_sk(stats->estats_sk);
+	s32 val;
+
+	if (tp->ecn_flags & TCP_ECN_OK)
+		val = 1;
+	else
+		val = sysctl_tcp_ecn ? 3 : 2;
+	memcpy(buf, &val, 4);
+}
+
+static void read_TimeStamps(void *buf, struct tcp_estats *stats,
+			    struct tcp_estats_var *vp)
+{
+	struct tcp_sock *tp = tcp_sk(stats->estats_sk);
+	s32 val;
+
+	if (tp->rx_opt.tstamp_ok)
+		val = 1;
+	else
+		val = sysctl_tcp_timestamps ? 3 : 2;
+	memcpy(buf, &val, 4);
+}
+
+static void read_MSSSent(void *buf, struct tcp_estats *stats,
+			 struct tcp_estats_var *vp)
+{
+	struct tcp_sock *tp = tcp_sk(stats->estats_sk);
+	u32 val;
+
+	val = tp->advmss;
+	memcpy(buf, &val, 4);
+}
+
+static void read_MSSRcvd(void *buf, struct tcp_estats *stats,
+			 struct tcp_estats_var *vp)
+{
+	struct tcp_sock *tp = tcp_sk(stats->estats_sk);
+	u32 val;
+
+	val = tp->rx_opt.rec_mss;
+	memcpy(buf, &val, 4);
+}
+
+/* Note: WinScaleSent and WinScaleRcvd are incorrectly
+ * implemented for the case where we sent a scale option
+ * but did not receive one. */
+static void read_WinScaleSent(void *buf, struct tcp_estats *stats,
+			      struct tcp_estats_var *vp)
+{
+	struct tcp_sock *tp = tcp_sk(stats->estats_sk);
+	s32 val;
+
+	val = tp->rx_opt.wscale_ok ? tp->rx_opt.rcv_wscale : -1;
+	memcpy(buf, &val, 4);
+}
+
+static void read_WinScaleRcvd(void *buf, struct tcp_estats *stats,
+			      struct tcp_estats_var *vp)
+{
+	struct tcp_sock *tp = tcp_sk(stats->estats_sk);
+	s32 val;
+
+	val = tp->rx_opt.wscale_ok ? tp->rx_opt.snd_wscale : -1;
+	memcpy(buf, &val, 4);
+}
+
+static void read_CurAppWQueue(void *buf, struct tcp_estats *stats,
+			      struct tcp_estats_var *vp)
+{
+	struct tcp_sock *tp = tcp_sk(stats->estats_sk);
+	u32 val = tp->write_seq - stats->estats_vars.SndMax;
+
+	memcpy(buf, &val, 4);
+}
+
+static void read_CurAppRQueue(void *buf, struct tcp_estats *stats,
+			      struct tcp_estats_var *vp)
+{
+	struct tcp_sock *tp = tcp_sk(stats->estats_sk);
+	u32 val = tp->rcv_nxt - tp->copied_seq;
+
+	memcpy(buf, &val, 4);
+}
+
+static void read_CurReasmQueue(void *buf, struct tcp_estats *stats,
+			       struct tcp_estats_var *vp)
+{
+	struct tcp_sock *tp = tcp_sk(stats->estats_sk);
+	u32 val = ofo_qlen(tp);
+
+	memcpy(buf, &val, 4);
+}
+
+/*
+ * Table of exported MIB variables
+ */
+#define OFFSET_IN(type,var)	((unsigned long)(&(((type *)NULL)->var)))
+#define OFFSET_ST(field)	((unsigned long)(&(((struct tcp_estats *)NULL)->estats_vars.field)))
+#define OFFSET_SK(field)	((unsigned long)(&(((struct sock *)NULL)->field)))
+#define OFFSET_TP(field)	((unsigned long)(&(((struct tcp_sock *)NULL)->field)))
+#define STATSVAR(__name,__type)		{ .name = #__name, .type = TCP_ESTATS_TYPE_##__type, .read = read_stats, .read_data = OFFSET_ST(__name), .write = NULL }
+#define STATSVARN(__name,__type,__var)	{ .name = #__name, .type = TCP_ESTATS_TYPE_##__type, .read = read_stats, .read_data = OFFSET_ST(__var), .write = NULL }
+#define TPVAR32(__name,__type,__var)	{ .name = #__name, .type = TCP_ESTATS_TYPE_##__type, .read = read_sk32, .read_data = OFFSET_TP(__var), .write = NULL }
+#define READFUN(__name,__type)		{ .name = #__name, .type = TCP_ESTATS_TYPE_##__type, .read = read_##__name, .write = NULL }
+#define RWFUN(__name,__type)		{ .name = #__name, .type = TCP_ESTATS_TYPE_##__type, .read = read_##__name, .write = write_##__name }
+
+struct tcp_estats_var tcp_estats_var_table[] = {
+	/* Connection table */
+	STATSVAR(LocalAddressType, INTEGER),
+	STATSVAR(LocalAddress, INET_ADDRESS),
+	STATSVAR(LocalPort, INET_PORT_NUMBER),
+	STATSVARN(RemAddressType, INTEGER, LocalAddressType),
+	STATSVAR(RemAddress, INET_ADDRESS),
+	STATSVAR(RemPort, INET_PORT_NUMBER),
+
+	/* Perf table */
+	STATSVAR(SegsOut, COUNTER32),
+	STATSVAR(DataSegsOut, COUNTER32),
+	STATSVAR(DataOctetsOut, COUNTER64),
+	STATSVAR(SegsRetrans, COUNTER32),
+	STATSVAR(OctetsRetrans, COUNTER32),
+	STATSVAR(SegsIn, COUNTER32),
+	STATSVAR(DataSegsIn, COUNTER32),
+	STATSVAR(DataOctetsIn, COUNTER64),
+	READFUN(ElapsedSecs, COUNTER32),
+	READFUN(ElapsedMicroSecs, COUNTER32),
+	/* StartTimeStamp - Not implemented.
+	 * See StartTimeSecs and StartTimeMicroSecs below. */
+	TPVAR32(CurMSS, GAUGE32, mss_cache),
+	READFUN(PipeSize, GAUGE32),
+	STATSVAR(MaxPipeSize, GAUGE32),
+	READFUN(SmoothedRTT, GAUGE32),
+	READFUN(CurRTO, GAUGE32),
+	STATSVAR(CongSignals, COUNTER32),
+	READFUN(CurCwnd, GAUGE32),
+	READFUN(CurSsthresh, GAUGE32),
+	STATSVAR(Timeouts, COUNTER32),
+	TPVAR32(CurRwinSent, GAUGE32, rcv_wnd),
+	STATSVAR(MaxRwinSent, GAUGE32),
+	STATSVAR(ZeroRwinSent, GAUGE32),
+	TPVAR32(CurRwinRcvd, GAUGE32, snd_wnd),
+	STATSVAR(MaxRwinRcvd, GAUGE32),
+	STATSVAR(ZeroRwinRcvd, GAUGE32),
+	STATSVARN(SndLimTransRwin, COUNTER32,
+		  snd_lim_trans[TCP_ESTATS_SNDLIM_RWIN]),
+	STATSVARN(SndLimTransCwnd, COUNTER32,
+		  snd_lim_trans[TCP_ESTATS_SNDLIM_CWND]),
+	STATSVARN(SndLimTransSnd, COUNTER32,
+		  snd_lim_trans[TCP_ESTATS_SNDLIM_SENDER]),
+	STATSVARN(SndLimTimeRwin, COUNTER32,
+		  snd_lim_time[TCP_ESTATS_SNDLIM_RWIN]),
+	STATSVARN(SndLimTimeCwnd, COUNTER32,
+		  snd_lim_time[TCP_ESTATS_SNDLIM_CWND]),
+	STATSVARN(SndLimTimeSnd, COUNTER32,
+		  snd_lim_time[TCP_ESTATS_SNDLIM_SENDER]),
+	STATSVAR(SendStall, COUNTER32),
+
+	/* Path table */
+	READFUN(RetranThresh, GAUGE32),
+	STATSVAR(NonRecovDAEpisodes, COUNTER32),
+	STATSVAR(SumOctetsReordered, COUNTER32),
+	STATSVAR(NonRecovDA, COUNTER32),
+	STATSVAR(SampleRTT, GAUGE32),
+	READFUN(RTTVar, GAUGE32),
+	STATSVAR(MaxRTT, GAUGE32),
+	STATSVAR(MinRTT, GAUGE32),
+	STATSVAR(SumRTT, COUNTER64),
+	STATSVAR(CountRTT, COUNTER32),
+	STATSVAR(MaxRTO, GAUGE32),
+	STATSVAR(MinRTO, GAUGE32),
+	STATSVAR(IpTtl, OCTET),
+	STATSVAR(IpTosIn, OCTET),
+	READFUN(IpTosOut, OCTET),
+	STATSVAR(PreCongSumCwnd, COUNTER32),
+	STATSVAR(PreCongSumRTT, COUNTER32),
+	STATSVAR(PostCongSumRTT, COUNTER32),
+	STATSVAR(PostCongCountRTT, COUNTER32),
+	STATSVAR(ECNsignals, COUNTER32),
+	STATSVAR(DupAckEpisodes, COUNTER32),
+	READFUN(RcvRTT, GAUGE32),
+	STATSVAR(DupAcksOut, COUNTER32),
+	STATSVAR(CERcvd, COUNTER32),
+	STATSVAR(ECESent, COUNTER32),
+
+	/* Stack table */
+	STATSVAR(ActiveOpen, INTEGER),
+	READFUN(MSSSent, UNSIGNED32),
+	READFUN(MSSRcvd, UNSIGNED32),
+	READFUN(WinScaleSent, INTEGER32),
+	READFUN(WinScaleRcvd, INTEGER32),
+	READFUN(TimeStamps, INTEGER),
+	READFUN(ECN, INTEGER),
+	READFUN(WillSendSACK, INTEGER),
+	READFUN(WillUseSACK, INTEGER),
+	RWFUN(State, INTEGER),
+	READFUN(Nagle, INTEGER),
+	STATSVAR(MaxSsCwnd, GAUGE32),
+	STATSVAR(MaxCaCwnd, GAUGE32),
+	STATSVAR(MaxSsthresh, GAUGE32),
+	STATSVAR(MinSsthresh, GAUGE32),
+	READFUN(InRecovery, INTEGER),
+	STATSVAR(DupAcksIn, COUNTER32),
+	STATSVAR(SpuriousFrDetected, COUNTER32),
+	STATSVAR(SpuriousRtoDetected, COUNTER32),
+	STATSVAR(SoftErrors, COUNTER32),
+	STATSVAR(SoftErrorReason, INTEGER),
+	STATSVAR(SlowStart, COUNTER32),
+	STATSVAR(CongAvoid, COUNTER32),
+	STATSVAR(OtherReductions, COUNTER32),
+	STATSVAR(CongOverCount, COUNTER32),
+	STATSVAR(FastRetran, COUNTER32),
+	STATSVAR(SubsequentTimeouts, COUNTER32),
+	READFUN(CurTimeoutCount, GAUGE32),
+	STATSVAR(AbruptTimeouts, COUNTER32),
+	STATSVAR(SACKsRcvd, COUNTER32),
+	STATSVAR(SACKBlocksRcvd, COUNTER32),
+	STATSVAR(DSACKDups, COUNTER32),
+	STATSVAR(MaxMSS, GAUGE32),
+	STATSVAR(MinMSS, GAUGE32),
+	STATSVAR(SndInitial, UNSIGNED32),
+	STATSVAR(RecInitial, UNSIGNED32),
+	STATSVAR(CurRetxQueue, GAUGE32),
+	STATSVAR(MaxRetxQueue, GAUGE32),
+	READFUN(CurReasmQueue, GAUGE32),
+	STATSVAR(MaxReasmQueue, GAUGE32),
+
+	/* App table */
+	TPVAR32(SndUna, COUNTER32, snd_una),
+	TPVAR32(SndNxt, UNSIGNED32, snd_nxt),
+	STATSVAR(SndMax, COUNTER32),
+	STATSVAR(ThruOctetsAcked, COUNTER64),
+	TPVAR32(RcvNxt, COUNTER32, rcv_nxt),
+	STATSVAR(ThruOctetsReceived, COUNTER64),
+	READFUN(CurAppWQueue, GAUGE32),
+	STATSVAR(MaxAppWQueue, GAUGE32),
+	READFUN(CurAppRQueue, GAUGE32),
+	STATSVAR(MaxAppRQueue, GAUGE32),
+
+	/* Tune table */
+	RWFUN(LimCwnd, GAUGE32),
+	/* We can't write LimSsthresh for now because it is only a
+	 * a sysctl.  Maybe add per-connection variable later. */
+	READFUN(LimSsthresh, GAUGE32),
+	{
+	 .name = "LimRwin",
+	 .type = TCP_ESTATS_TYPE_GAUGE32,
+	 .read = read_sk32,
+	 .read_data = OFFSET_TP(window_clamp),
+	 .write = write_LimRwin,
+	 },
+	{
+	 .name = "LimMSS",
+	 .type = TCP_ESTATS_TYPE_GAUGE32,
+	 .read = read_sk32,
+	 .read_data = OFFSET_TP(rx_opt.mss_clamp),
+	 .write = write_LimMSS,
+	 },
+
+	/* Extras (non-standard) */
+	STATSVAR(OtherReductionsCV, COUNTER32),
+	STATSVAR(OtherReductionsCM, COUNTER32),
+	READFUN(StartTimeSecs, UNSIGNED32),
+	READFUN(StartTimeMicroSecs, UNSIGNED32),
+	{
+	 .name = "Sndbuf",
+	 .type = TCP_ESTATS_TYPE_GAUGE32,
+	 .read = read_sk32,
+	 .read_data = OFFSET_SK(sk_sndbuf),
+	 .write = write_Sndbuf,
+	 },
+	{
+	 .name = "Rcvbuf",
+	 .type = TCP_ESTATS_TYPE_GAUGE32,
+	 .read = read_sk32,
+	 .read_data = OFFSET_SK(sk_rcvbuf),
+	 .write = write_Rcvbuf,
+	 },
+	{.name = NULL}
+};
+
+void __init tcp_estats_init()
+{
+	int order;
+	int i;
+
+	tcp_estats_ht =
+	    (struct list_head *)alloc_large_system_hash("TCP ESTATS",
+							sizeof (struct list_head),
+							tcp_hashinfo.ehash_mask + 1,
+							(totalram_pages >= 128 * 1024) ?
+							13 : 15,
+							0,
+							&order, NULL,
+							64 * 1024);
+	tcp_estats_htsize = 1 << order;
+	for (i = 0; i < tcp_estats_htsize; i++)
+		INIT_LIST_HEAD(&tcp_estats_ht[i]);
+
+	if ((tcp_estats_head = kmalloc(sizeof (struct list_head), GFP_KERNEL)) == NULL) {
+		printk(KERN_ERR "tcp_estats_init(): kmalloc failed\n");
+		goto cleanup_fail;
+	}
+	INIT_LIST_HEAD(tcp_estats_head);
+
+	if (tcp_estats_proc_init())
+		goto cleanup_fail;
+
+	tcp_estats_enabled = 1;
+	return;
+
+      cleanup_fail:
+	free_pages((unsigned long)tcp_estats_ht, order);
+	printk("TCP ESTATS: initialization failed.\n");
+}
+
+#ifdef CONFIG_IPV6_MODULE
+EXPORT_SYMBOL(tcp_estats_create);
+EXPORT_SYMBOL(tcp_estats_update_segrecv);
+EXPORT_SYMBOL(tcp_estats_update_finish_segrecv);
+#endif
diff --git a/net/ipv4/tcp_estats_proc.c b/net/ipv4/tcp_estats_proc.c
new file mode 100644
index 0000000..72cc018
--- /dev/null
+++ b/net/ipv4/tcp_estats_proc.c
@@ -0,0 +1,834 @@
+/*
+ *  fs/proc/tcp_estats_proc.c
+ *
+ * Authors:
+ *   John Heffner <jheffner@psc.edu>
+ *   Matt Mathis <mathis@psc.edu>
+ *   Jeff Semke <semke@psc.edu>
+ *
+ * The Web10Gig project.  See http://www.web10gig.org
+ *
+ * Copyright Â© 2011, Pittsburgh Supercomputing Center (PSC).
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ *
+ */
+
+#include <linux/proc_fs.h>
+#include <linux/init.h>
+#include <linux/sysctl.h>
+#include <linux/mount.h>
+#include <linux/list.h>
+#include <linux/pid_namespace.h>
+#include <net/sock.h>
+#include <net/tcp.h>
+#include <net/tcp_estats.h>
+
+#include "../../fs/proc/internal.h"
+
+extern __u32 sysctl_wmem_default;
+extern __u32 sysctl_wmem_max;
+
+struct proc_dir_entry *proc_tcp_estats_dir;
+static struct proc_dir_entry *proc_tcp_estats_header;
+
+/*
+ * ESTATS variable reading/writing
+ */
+
+enum tcp_estats_connection_inos {
+	PROC_CONN_SPEC_ASCII = 1,
+	PROC_CONN_SPEC,
+	PROC_CONN_READ,
+	PROC_CONN_TUNE,
+	PROC_CONN_HIGH_INO	/* Keep at the end */
+};
+
+struct estats_file {
+	char *name;
+	int len;
+	int low_ino;
+	mode_t mode;
+};
+
+#define F(name,ino,perm) { (name), sizeof (name) - 1, (ino), (perm) }
+static struct estats_file estats_file_arr[] = {
+	F("spec-ascii", PROC_CONN_SPEC_ASCII, S_IFREG | S_IRUGO),
+	F("spec", PROC_CONN_SPEC, S_IFREG | S_IRUGO),
+	F("read", PROC_CONN_READ, 0),
+#if 0
+	F("tune", PROC_CONN_TUNE, 0),
+#endif
+	F(NULL, 0, 0)
+};
+
+#define ESTATS_FILE_ARR_SIZE	(sizeof (estats_file_arr) / sizeof (struct estats_file))
+
+static struct estats_file *file_spec_ascii = &estats_file_arr[0];
+static struct estats_file *file_spec = &estats_file_arr[1];
+
+/* This works only if the array is built in the correct order. */
+static inline struct estats_file *estats_file_lookup(int ino)
+{
+	return &estats_file_arr[ino - 1];
+}
+
+/*
+ * proc filesystem routines
+ */
+
+static struct inode *proc_estats_make_inode(struct super_block *sb, int ino)
+{
+	struct inode *inode;
+
+	inode = new_inode(sb);
+	if (!inode)
+		goto out;
+
+	inode->i_mtime = inode->i_atime = inode->i_ctime = CURRENT_TIME;
+	inode->i_ino = ino;
+
+	inode->i_uid = 0;
+	inode->i_gid = 0;
+
+      out:
+	return inode;
+}
+
+static inline ino_t ino_from_cid(int cid)
+{
+	return (cid << 8) | 0x80000000;
+}
+
+static inline ino_t ino_from_parts(ino_t dir_ino, __u16 low_ino)
+{
+	return (dir_ino & ~0xff) | low_ino;
+}
+
+static inline int cid_from_ino(ino_t ino)
+{
+	return (ino & 0x7fffff00) >> 8;
+}
+
+static inline int low_from_ino(ino_t ino)
+{
+	return ino & 0xff;
+}
+
+static int connection_file_open(struct inode *inode, struct file *file)
+{
+	int cid = cid_from_ino(inode->i_ino);
+	struct tcp_estats *stats;
+
+	read_lock_bh(&tcp_estats_linkage_lock);
+	stats = tcp_estats_lookup(cid);
+	if (stats == NULL || stats->estats_dead) {
+		read_unlock_bh(&tcp_estats_linkage_lock);
+		return -ENOENT;
+	}
+	tcp_estats_use(stats);
+	read_unlock_bh(&tcp_estats_linkage_lock);
+
+	return 0;
+}
+
+static int connection_file_release(struct inode *inode, struct file *file)
+{
+	int cid = cid_from_ino(inode->i_ino);
+	struct tcp_estats *stats;
+
+	read_lock_bh(&tcp_estats_linkage_lock);
+	stats = tcp_estats_lookup(cid);
+	if (stats == NULL) {
+		read_unlock_bh(&tcp_estats_linkage_lock);
+		return -ENOENT;
+	}
+	read_unlock_bh(&tcp_estats_linkage_lock);
+	tcp_estats_unuse(stats);
+
+	return 0;
+}
+
+/**  <base>/<connection>/<binary variable files>  **/
+static ssize_t connection_file_rw(int read, struct file *file,
+				  char *buf, size_t nbytes, loff_t *ppos)
+{
+	int low_ino = low_from_ino(file->f_dentry->d_inode->i_ino);
+	int cid = cid_from_ino(file->f_dentry->d_inode->i_ino);
+	struct tcp_estats *stats;
+	struct estats_file *fp;
+	struct tcp_estats_var *vp;
+	int pos;
+	int n;
+	int err;
+	estats_rwfunc_t rwfunc;
+	char *page;
+
+	/* We're only going to let them read one page at a time.
+	 * We shouldn't ever read more than a page, anyway, though.
+	 */
+	if (nbytes > PAGE_SIZE)
+		nbytes = PAGE_SIZE;
+
+	if (!access_ok(read ? VERIFY_WRITE : VERIFY_READ, buf, nbytes))
+		return -EFAULT;
+
+	if ((page = (char *)__get_free_page(GFP_KERNEL)) == NULL)
+		return -ENOMEM;
+
+	if (!read) {
+		if (copy_from_user(page, buf, nbytes))
+			return -EFAULT;
+	}
+
+	fp = estats_file_lookup(low_ino);
+	if (fp == NULL) {
+		printk(KERN_INFO "Unregistered ETSTATS file.\n");
+		return 0;
+	}
+
+	read_lock_bh(&tcp_estats_linkage_lock);
+	stats = tcp_estats_lookup(cid);
+	read_unlock_bh(&tcp_estats_linkage_lock);
+	if (stats == NULL)
+		return -ENOENT;
+
+	lock_sock(stats->estats_sk);
+
+	pos = 0;
+	n = 0;
+	for (vp = &tcp_estats_var_table[0]; vp->name && nbytes > n; vp++) {
+		int varlen = tcp_estats_var_len(vp);
+
+		if (pos > *ppos) {
+			err = -ESPIPE;
+			goto err_out;
+		}
+		if (pos == *ppos) {
+			if (varlen > nbytes - n)
+				break;
+
+			if (read)
+				rwfunc = vp->read;
+			else
+				rwfunc = vp->write;
+			if (rwfunc == NULL) {
+				err = -EACCES;
+				goto err_out;
+			}
+			rwfunc(page + n, stats, vp);
+			n += varlen;
+			*ppos += varlen;
+		}
+		pos += varlen;
+	}
+
+	release_sock(stats->estats_sk);
+
+	if (read) {
+		if (copy_to_user(buf, page, n))
+			return -EFAULT;
+	}
+	free_page((unsigned long)page);
+
+	return n;
+
+      err_out:
+	release_sock(stats->estats_sk);
+
+	return err;
+}
+
+static ssize_t connection_file_read(struct file *file,
+				    char *buf, size_t nbytes, loff_t *ppos)
+{
+	return connection_file_rw(1, file, buf, nbytes, ppos);
+}
+
+static ssize_t connection_file_write(struct file *file,
+				     const char *buf, size_t nbytes,
+				     loff_t *ppos)
+{
+	return connection_file_rw(0, file, (char *)buf, nbytes, ppos);
+}
+
+static struct file_operations connection_file_fops = {
+	.open = connection_file_open,
+	.release = connection_file_release,
+	.read = connection_file_read,
+	.write = connection_file_write,
+	.llseek = default_llseek
+};
+
+static size_t v6addr_str(char *dest, short *addr)
+{
+	int start = -1, end = -1;
+	int i, j;
+	int pos;
+
+	/* Find longest subsequence of 0's in addr */
+	for (i = 0; i < 8; i++) {
+		if (addr[i] == 0) {
+			for (j = i + 1; addr[j] == 0 && j < 8; j++) ;
+			if (j - i > end - start) {
+				end = j;
+				start = i;
+			}
+			i = j;
+		}
+	}
+	if (end - start == 1)
+		start = -1;
+
+	pos = 0;
+	for (i = 0; i < 8; i++) {
+		if (i > 0)
+			pos += sprintf(dest + pos, ":");
+		if (i == start) {
+			pos += sprintf(dest + pos, ":");
+			i += end - start - 1;
+		} else {
+			pos += sprintf(dest + pos, "%hx", ntohs(addr[i]));
+		}
+	}
+
+	return pos;
+}
+
+/**  <base>/<connection>/spec_ascii  **/
+static ssize_t connection_spec_ascii_read(struct file *file, char *buf,
+					  size_t nbytes, loff_t * ppos)
+{
+	u32 local_addr, remote_addr;
+	u16 local_port, remote_port;
+	int cid;
+	struct tcp_estats *stats;
+	struct tcp_estats_directs *vars;
+	char tmpbuf[100];
+	int len = 0;
+
+	if (*ppos != 0)
+		return 0;
+
+	cid = cid_from_ino(file->f_dentry->d_parent->d_inode->i_ino);
+
+	read_lock_bh(&tcp_estats_linkage_lock);
+	stats = tcp_estats_lookup(cid);
+	read_unlock_bh(&tcp_estats_linkage_lock);
+	if (stats == NULL)
+		return -ENOENT;
+	vars = &stats->estats_vars;
+
+	if (vars->LocalAddressType == TCP_ESTATS_ADDRTYPE_IPV4) {
+		/* These values should not change while stats are linked.
+		 * We don't need to lock the sock. */
+		memcpy(&local_addr, &vars->LocalAddress, 4);
+		local_addr = ntohl(local_addr);
+		memcpy(&remote_addr, &vars->RemAddress, 4);
+		remote_addr = ntohl(remote_addr);
+		local_port = vars->LocalPort;
+		remote_port = vars->RemPort;
+
+		len = sprintf(tmpbuf, "%d.%d.%d.%d:%d %d.%d.%d.%d:%d\n",
+			      (local_addr >> 24) & 0xff,
+			      (local_addr >> 16) & 0xff,
+			      (local_addr >> 8) & 0xff,
+			      local_addr & 0xff,
+			      local_port,
+			      (remote_addr >> 24) & 0xff,
+			      (remote_addr >> 16) & 0xff,
+			      (remote_addr >> 8) & 0xff,
+			      remote_addr & 0xff, remote_port);
+	} else if (vars->LocalAddressType == TCP_ESTATS_ADDRTYPE_IPV6) {
+		local_port = vars->LocalPort;
+		remote_port = vars->RemPort;
+
+		len += v6addr_str(tmpbuf + len, (short *)&vars->LocalAddress);
+		len += sprintf(tmpbuf + len, ".%d ", local_port);
+		len += v6addr_str(tmpbuf + len, (short *)&vars->RemAddress);
+		len += sprintf(tmpbuf + len, ".%d\n", remote_port);
+	} else {
+		printk(KERN_ERR
+		       "connection_spec_ascii_read: LocalAddressType invalid\n");
+		return 0;
+	}
+
+	len = len > nbytes ? nbytes : len;
+	if (copy_to_user(buf, tmpbuf, len))
+		return -EFAULT;
+	*ppos += len;
+	return len;
+}
+
+static struct file_operations connection_spec_ascii_fops = {
+	.open = connection_file_open,
+	.release = connection_file_release,
+	.read = connection_spec_ascii_read
+};
+
+/**  <base>/<connection>/  **/
+static int connection_dir_readdir(struct file *filp,
+				  void *dirent, filldir_t filldir)
+{
+	int i;
+	struct inode *inode = filp->f_dentry->d_inode;
+	struct estats_file *fp;
+
+	i = filp->f_pos;
+	switch (i) {
+	case 0:
+		if (filldir(dirent, ".", 1, i, inode->i_ino, DT_DIR) < 0)
+			return 0;
+		i++;
+		filp->f_pos++;
+		/* fall through */
+	case 1:
+		if (filldir
+		    (dirent, "..", 2, i, proc_tcp_estats_dir->low_ino,
+		     DT_DIR) < 0)
+			return 0;
+		i++;
+		filp->f_pos++;
+		/* fall through */
+	default:
+		i -= 2;
+		if (i >= ESTATS_FILE_ARR_SIZE)
+			return 1;
+		for (fp = &estats_file_arr[i]; fp->name; fp++) {
+			if (filldir(dirent, fp->name, fp->len, filp->f_pos,
+				    ino_from_parts(inode->i_ino, fp->low_ino),
+				    fp->mode >> 12) < 0)
+				return 0;
+			filp->f_pos++;
+		}
+	}
+
+	return 1;
+}
+
+static struct dentry *connection_dir_lookup(struct inode *dir,
+					    struct dentry *dentry,
+					    struct nameidata *nd)
+{
+	struct inode *inode;
+	struct estats_file *fp;
+	struct tcp_estats *stats;
+	uid_t uid;
+
+	inode = NULL;
+	for (fp = &estats_file_arr[0]; fp->name; fp++) {
+		if (fp->len != dentry->d_name.len)
+			continue;
+		if (!memcmp(dentry->d_name.name, fp->name, fp->len))
+			break;
+	}
+	if (!fp->name)
+		return ERR_PTR(-ENOENT);
+
+	read_lock_bh(&tcp_estats_linkage_lock);
+	if ((stats = tcp_estats_lookup(cid_from_ino(dir->i_ino))) == NULL) {
+		read_unlock_bh(&tcp_estats_linkage_lock);
+		printk(KERN_ERR "connection_dir_lookup: stats == NULL\n");
+		return ERR_PTR(-ENOENT);
+	}
+	uid = sock_i_uid(stats->estats_sk);
+	read_unlock_bh(&tcp_estats_linkage_lock);
+
+	inode =
+	    proc_estats_make_inode(dir->i_sb,
+				   ino_from_parts(dir->i_ino, fp->low_ino));
+	if (!inode)
+		return ERR_PTR(-ENOMEM);
+	inode->i_mode =
+	    fp->mode ? fp->mode : S_IFREG | sysctl_tcp_estats_fperms;
+	inode->i_uid = uid;
+	inode->i_gid = sysctl_tcp_estats_gid;
+
+	switch (fp->low_ino) {
+	case PROC_CONN_SPEC_ASCII:
+		inode->i_fop = &connection_spec_ascii_fops;
+		break;
+	case PROC_CONN_SPEC:
+	case PROC_CONN_READ:
+	case PROC_CONN_TUNE:
+		inode->i_fop = &connection_file_fops;
+		break;
+	default:
+		printk(KERN_INFO "TCP ESTATS: impossible type (%d)\n",
+		       fp->low_ino);
+		iput(inode);
+		return ERR_PTR(-EINVAL);
+	}
+
+	d_add(dentry, inode);
+	return NULL;
+}
+
+static struct inode_operations connection_dir_iops = {
+	.lookup = connection_dir_lookup
+};
+
+static struct file_operations connection_dir_fops = {
+	.readdir = connection_dir_readdir
+};
+
+/**  <base>/header  **/
+static ssize_t header_read(struct file *file, char *buf,
+			   size_t nbytes, loff_t * ppos)
+{
+	int len = 0;
+	loff_t offset;
+	char *tmpbuf;
+	struct estats_file *fp;
+	struct tcp_estats_var *vp;
+	int n, tmp, i;
+	int ret = 0;
+
+	if ((tmpbuf = (char *)__get_free_page(GFP_KERNEL)) == NULL)
+		return -ENOMEM;
+
+	/* Web100 version string for backward compatibility,
+	 * doesn't really apply anymore */
+	offset = sprintf(tmpbuf, "3.0 0\n");
+
+	for (fp = &estats_file_arr[0]; fp->name; fp++) {
+		int file_offset = 0;
+
+		if (fp == file_spec_ascii)
+			continue;
+
+		offset += sprintf(tmpbuf + offset, "\n/%s\n", fp->name);
+
+		for (i = 0, vp = &tcp_estats_var_table[0]; vp->name; vp++, i++) {
+			int varlen = tcp_estats_var_len(vp);
+
+			/* Hack alert */
+			if (fp == file_spec && i > 5)
+				break;
+
+			if (offset > PAGE_SIZE - 1024) {
+				len += offset;
+				if (*ppos < len) {
+					n = min(offset,
+						min_t(loff_t, nbytes,
+						      len - *ppos));
+					if (copy_to_user
+					    (buf,
+					     tmpbuf + max_t(loff_t,
+							    *ppos - len +
+							    offset, 0), n))
+						return -EFAULT;
+					buf += n;
+					if (nbytes == n) {
+						*ppos += n;
+						ret = n;
+						goto out;
+					}
+				}
+				offset = 0;
+			}
+
+			offset += sprintf(tmpbuf + offset, "%s %d %d %d\n",
+					  vp->name, file_offset, vp->type,
+					  varlen);
+			file_offset += varlen;
+		}
+	}
+	len += offset;
+	if (*ppos < len) {
+		n = min(offset, min_t(loff_t, nbytes, len - *ppos));
+		if (copy_to_user
+		    (buf, tmpbuf + max_t(loff_t, *ppos - len + offset, 0), n))
+			return -EFAULT;
+		if (nbytes <= len - *ppos) {
+			*ppos += nbytes;
+			ret = nbytes;
+			goto out;
+		} else {
+			tmp = len - *ppos;
+			*ppos = len;
+			ret = tmp;
+			goto out;
+		}
+	}
+
+      out:
+	free_page((unsigned long)tmpbuf);
+	return ret;
+}
+
+static struct file_operations header_file_operations = {
+	.read = header_read
+};
+
+/**  <base>/  **/
+#define FIRST_CONNECTION_ENTRY	256
+#define NUMBUF_LEN		11
+
+static int get_connection_list(int pos, int *cids, int max)
+{
+	struct list_head *p;
+	int n;
+
+	pos -= FIRST_CONNECTION_ENTRY;
+	n = 0;
+
+	read_lock_bh(&tcp_estats_linkage_lock);
+
+	list_for_each(p, tcp_estats_head) {
+		struct tcp_estats *stats = list_entry(p, struct tcp_estats, estats_list);
+		
+		if (n >= max)
+			break;
+		if (!stats->estats_dead) {
+			if (pos <= 0)
+				cids[n++] = stats->estats_cid;
+			else
+				pos--;
+		}
+	}
+
+	read_unlock_bh(&tcp_estats_linkage_lock);
+
+	return n;
+}
+
+static int cid_to_str(int cid, char *buf)
+{
+	int len, tmp, i;
+
+	if (cid == 0) {		/* a special case */
+		len = 1;
+	} else {
+		tmp = cid;
+		for (len = 0; len < NUMBUF_LEN - 1 && tmp > 0; len++)
+			tmp /= 10;
+	}
+
+	for (i = 0; i < len; i++) {
+		buf[len - i - 1] = '0' + (cid % 10);
+		cid /= 10;
+	}
+	buf[len] = '\0';
+
+	return len;
+}
+
+static int estats_dir_readdir(struct file *filp,
+			      void *dirent, filldir_t filldir)
+{
+	int err;
+	unsigned n, i;
+	int *cids;
+	int len;
+	ino_t ino;
+	char name[NUMBUF_LEN];
+	int n_conns;
+
+	if (filp->f_pos < FIRST_CONNECTION_ENTRY) {
+		if ((err = proc_readdir(filp, dirent, filldir)) < 0)
+			return err;
+		filp->f_pos = FIRST_CONNECTION_ENTRY;
+	}
+	n_conns = (tcp_estats_conn_num + 2) * 2;
+	do {
+		n_conns /= 2;
+		cids = kmalloc(n_conns * sizeof(int), GFP_KERNEL);
+	} while (cids == NULL && n_conns > 0);
+	if (cids == NULL)
+		return -ENOMEM;
+	n = get_connection_list(filp->f_pos, cids, n_conns);
+
+	for (i = 0; i < n; i++) {
+		ino = ino_from_cid(cids[i]);
+		len = cid_to_str(cids[i], name);
+		if (filldir(dirent, name, len, filp->f_pos, ino, DT_DIR) < 0) {
+			break;
+		}
+		filp->f_pos++;
+	}
+
+	kfree(cids);
+
+	return 0;
+}
+
+static inline struct dentry *estats_dir_dent(void)
+{
+	struct qstr qstr;
+	struct vfsmount *mnt = current->nsproxy->pid_ns->proc_mnt;
+
+	qstr.name = "web100";
+	qstr.len = 6;
+	qstr.hash = full_name_hash(qstr.name, qstr.len);
+
+	return d_lookup(mnt->mnt_sb->s_root, &qstr);
+}
+
+void estats_proc_nlink_update(nlink_t nlink)
+{
+	struct dentry *dent;
+
+	dent = estats_dir_dent();
+	if (dent)
+		dent->d_inode->i_nlink = nlink;
+	dput(dent);
+}
+
+int tcp_estats_proc_dointvec_update(ctl_table * ctl, int write,
+				    void *buffer, size_t * lenp, 
+				    loff_t * ppos)
+{
+	unsigned n, i;
+	int *cids;
+	int err;
+	struct qstr qstr;
+	struct dentry *estats_dent, *conn_dent, *dent;
+	struct inode *inode;
+	struct estats_file *fp;
+	char name[NUMBUF_LEN];
+
+	if ((err = proc_dointvec(ctl, write, buffer, lenp, ppos)) != 0)
+		return err;
+
+	if ((estats_dent = estats_dir_dent()) == NULL)
+		return 0;
+
+	/* This is ugly and racy. */
+	if ((cids =
+	     kmalloc(tcp_estats_conn_num * sizeof(int), GFP_KERNEL)) == NULL)
+		return -ENOMEM;
+	n = get_connection_list(FIRST_CONNECTION_ENTRY, cids,
+				tcp_estats_conn_num);
+	for (i = 0; i < n; i++) {
+		qstr.len = cid_to_str(cids[i], name);
+		qstr.name = name;
+		qstr.hash = full_name_hash(qstr.name, qstr.len);
+		if ((conn_dent = d_lookup(estats_dent, &qstr)) != NULL) {
+			for (fp = &estats_file_arr[0]; fp->name; fp++) {
+				qstr.name = fp->name;
+				qstr.len = fp->len;
+				qstr.hash = full_name_hash(qstr.name, qstr.len);
+				if ((dent = d_lookup(conn_dent, &qstr)) != NULL) {
+					inode = dent->d_inode;
+					if ((inode->i_mode = fp->mode) == 0)
+						inode->i_mode =
+						    S_IFREG |
+						    sysctl_tcp_estats_fperms;
+					inode->i_gid = sysctl_tcp_estats_gid;
+					dput(dent);
+				}
+			}
+			dput(conn_dent);
+		}
+	}
+	dput(estats_dent);
+	kfree(cids);
+
+	return 0;
+}
+
+static int estats_proc_connection_revalidate(struct dentry *dentry,
+					     struct nameidata *nd)
+{
+	int ret = 1;
+
+	if (dentry->d_inode == NULL)
+		return 0;
+	read_lock_bh(&tcp_estats_linkage_lock);
+	if (tcp_estats_lookup(cid_from_ino(dentry->d_inode->i_ino)) == NULL) {
+		ret = 0;
+		d_drop(dentry);
+	}
+	read_unlock_bh(&tcp_estats_linkage_lock);
+
+	return ret;
+}
+
+static struct dentry_operations estats_dir_dentry_operations = {
+      d_revalidate:estats_proc_connection_revalidate
+};
+
+static struct dentry *estats_dir_lookup(struct inode *dir,
+					struct dentry *dentry,
+					struct nameidata *nd)
+{
+	char *name;
+	int len;
+	int cid;
+	unsigned c;
+	struct inode *inode;
+	unsigned long ino;
+	struct tcp_estats *stats;
+
+	if (proc_lookup(dir, dentry, nd) == NULL)
+		return NULL;
+
+	cid = 0;
+	name = (char *)(dentry->d_name.name);
+	len = dentry->d_name.len;
+	if (len <= 0)		/* I don't think this can happen */
+		return ERR_PTR(-EINVAL);
+	while (len-- > 0) {
+		c = *name - '0';
+		name++;
+		cid *= 10;
+		cid += c;
+		if (c > 9 || c < 0 || (cid == 0 && len != 0)) {
+			cid = -1;
+			break;
+		}
+	}
+	if (cid < 0)
+		return ERR_PTR(-ENOENT);
+
+	read_lock_bh(&tcp_estats_linkage_lock);
+	stats = tcp_estats_lookup(cid);
+	if (stats == NULL || stats->estats_dead) {
+		read_unlock_bh(&tcp_estats_linkage_lock);
+		return ERR_PTR(-ENOENT);
+	}
+	read_unlock_bh(&tcp_estats_linkage_lock);
+
+	ino = ino_from_cid(cid);
+	inode = proc_estats_make_inode(dir->i_sb, ino);
+	if (inode == NULL)
+		return ERR_PTR(-ENOMEM);
+	inode->i_nlink = 2;
+	inode->i_mode = S_IFDIR | S_IRUGO | S_IXUGO;
+	inode->i_flags |= S_IMMUTABLE;	/* ? */
+	inode->i_op = &connection_dir_iops;
+	inode->i_fop = &connection_dir_fops;
+
+	dentry->d_op = &estats_dir_dentry_operations;
+	d_add(dentry, inode);
+	return NULL;
+}
+
+static struct file_operations estats_dir_fops = {
+	.readdir = estats_dir_readdir
+};
+
+static struct inode_operations estats_dir_iops = {
+	.lookup = estats_dir_lookup
+};
+
+/*
+ * init
+ */
+
+int __init tcp_estats_proc_init(void)
+{
+	/* Set up the proc files. */
+	proc_tcp_estats_dir = proc_mkdir("web100", NULL);
+	proc_tcp_estats_dir->proc_iops = &estats_dir_iops;
+	proc_tcp_estats_dir->proc_fops = &estats_dir_fops;
+
+	proc_tcp_estats_header = create_proc_entry("header", S_IFREG | S_IRUGO,
+						   proc_tcp_estats_dir);
+	proc_tcp_estats_header->proc_fops = &header_file_operations;
+
+	return 0;
+}
diff --git a/net/ipv4/tcp_input.c b/net/ipv4/tcp_input.c
index bef9f04..e51facd 100644
--- a/net/ipv4/tcp_input.c
+++ b/net/ipv4/tcp_input.c
@@ -220,8 +220,12 @@ static inline void TCP_ECN_withdraw_cwr(struct tcp_sock *tp)
 static inline void TCP_ECN_check_ce(struct tcp_sock *tp, struct sk_buff *skb)
 {
 	if (tp->ecn_flags & TCP_ECN_OK) {
-		if (INET_ECN_is_ce(TCP_SKB_CB(skb)->flags))
+		if (INET_ECN_is_ce(TCP_SKB_CB(skb)->flags)) {
+			TCP_ESTATS_VAR_INC(tp, CERcvd);
+			if (tp->ecn_flags & TCP_ECN_DEMAND_CWR)
+				TCP_ESTATS_VAR_INC(tp, ECESent);
 			tp->ecn_flags |= TCP_ECN_DEMAND_CWR;
+		}
 		/* Funny extension: if ECT is not set on a segment,
 		 * it is surely retransmit. It is not in ECN RFC,
 		 * but Linux follows this rule. */
@@ -842,6 +846,7 @@ void tcp_enter_cwr(struct sock *sk, const int set_ssthresh)
 
 		tcp_set_ca_state(sk, TCP_CA_CWR);
 	}
+	TCP_ESTATS_UPDATE(tp, tcp_estats_update_congestion(tp));
 }
 
 /*
@@ -1728,6 +1733,9 @@ tcp_sacktag_write_queue(struct sock *sk, struct sk_buff *ack_skb,
 	state.flag = 0;
 	state.reord = tp->packets_out;
 
+	TCP_ESTATS_VAR_INC(tp, SACKsRcvd);
+	TCP_ESTATS_VAR_ADD(tp, SACKBlocksRcvd, num_sacks);
+
 	if (!tp->sacked_out) {
 		if (WARN_ON(tp->fackets_out))
 			tp->fackets_out = 0;
@@ -2203,6 +2211,8 @@ void tcp_enter_loss(struct sock *sk, int how)
 	struct tcp_sock *tp = tcp_sk(sk);
 	struct sk_buff *skb;
 
+	TCP_ESTATS_UPDATE(tp, tcp_estats_update_congestion(tp));
+
 	/* Reduce ssthresh if it has not yet been made inside this window. */
 	if (icsk->icsk_ca_state <= TCP_CA_Disorder || tp->snd_una == tp->high_seq ||
 	    (icsk->icsk_ca_state == TCP_CA_Loss && !icsk->icsk_retransmits)) {
@@ -2583,9 +2593,15 @@ static void tcp_update_scoreboard(struct sock *sk, int fast_rexmit)
  */
 static inline void tcp_moderate_cwnd(struct tcp_sock *tp)
 {
-	tp->snd_cwnd = min(tp->snd_cwnd,
-			   tcp_packets_in_flight(tp) + tcp_max_burst(tp));
-	tp->snd_cwnd_stamp = tcp_time_stamp;
+	u32 pkts = tcp_packets_in_flight(tp) + tcp_max_burst(tp);
+
+	if (pkts < tp->snd_cwnd) {
+		tp->snd_cwnd = pkts;
+		tp->snd_cwnd_stamp = tcp_time_stamp;
+
+		TCP_ESTATS_VAR_INC(tp, OtherReductions);
+		TCP_ESTATS_VAR_INC(tp, OtherReductionsCM);
+	}
 }
 
 /* Lower bound on congestion window is slow start threshold
@@ -2674,6 +2690,7 @@ static void tcp_undo_cwr(struct sock *sk, const bool undo_ssthresh)
 		if (undo_ssthresh && tp->prior_ssthresh > tp->snd_ssthresh) {
 			tp->snd_ssthresh = tp->prior_ssthresh;
 			TCP_ECN_withdraw_cwr(tp);
+			TCP_ESTATS_VAR_INC(tp, CongOverCount);
 		}
 	} else {
 		tp->snd_cwnd = max(tp->snd_cwnd, tp->snd_ssthresh);
@@ -2699,11 +2716,13 @@ static int tcp_try_undo_recovery(struct sock *sk)
 		 */
 		DBGUNDO(sk, inet_csk(sk)->icsk_ca_state == TCP_CA_Loss ? "loss" : "retrans");
 		tcp_undo_cwr(sk, true);
-		if (inet_csk(sk)->icsk_ca_state == TCP_CA_Loss)
+		if (inet_csk(sk)->icsk_ca_state == TCP_CA_Loss) {
 			mib_idx = LINUX_MIB_TCPLOSSUNDO;
-		else
+			TCP_ESTATS_VAR_INC(tp, SpuriousRtoDetected);
+		} else {
 			mib_idx = LINUX_MIB_TCPFULLUNDO;
-
+			TCP_ESTATS_VAR_INC(tp, SpuriousFrDetected);
+		}
 		NET_INC_STATS_BH(sock_net(sk), mib_idx);
 		tp->undo_marker = 0;
 	}
@@ -2852,8 +2871,10 @@ static void tcp_try_to_open(struct sock *sk, int flag)
 	if (!tp->frto_counter && !tcp_any_retrans_done(sk))
 		tp->retrans_stamp = 0;
 
-	if (flag & FLAG_ECE)
+	if (flag & FLAG_ECE) {
 		tcp_enter_cwr(sk, 1);
+		TCP_ESTATS_VAR_INC(tp, ECNsignals);
+	}
 
 	if (inet_csk(sk)->icsk_ca_state != TCP_CA_CWR) {
 		tcp_try_keep_open(sk);
@@ -3018,6 +3039,7 @@ static void tcp_fastretrans_alert(struct sock *sk, int pkts_acked, int flag)
 				tp->undo_marker = 0;
 				tcp_set_ca_state(sk, TCP_CA_Open);
 			}
+			TCP_ESTATS_VAR_INC(tp, NonRecovDAEpisodes);
 			break;
 
 		case TCP_CA_Recovery:
@@ -3060,8 +3082,10 @@ static void tcp_fastretrans_alert(struct sock *sk, int pkts_acked, int flag)
 				tcp_add_reno_sack(sk);
 		}
 
-		if (icsk->icsk_ca_state == TCP_CA_Disorder)
+		if (icsk->icsk_ca_state == TCP_CA_Disorder) {
 			tcp_try_undo_dsack(sk);
+			TCP_ESTATS_VAR_INC(tp, NonRecovDA);
+		}
 
 		if (!tcp_time_to_recover(sk)) {
 			tcp_try_to_open(sk, flag);
@@ -3104,6 +3128,8 @@ static void tcp_fastretrans_alert(struct sock *sk, int pkts_acked, int flag)
 		tp->snd_cwnd_cnt = 0;
 		tcp_set_ca_state(sk, TCP_CA_Recovery);
 		fast_rexmit = 1;
+		TCP_ESTATS_UPDATE(tp, tcp_estats_update_congestion(tp));
+		TCP_ESTATS_VAR_INC(tp, FastRetran);
 	}
 
 	if (do_lost || (tcp_is_fack(tp) && tcp_head_timedout(sk)))
@@ -3116,6 +3142,7 @@ static void tcp_valid_rtt_meas(struct sock *sk, u32 seq_rtt)
 {
 	tcp_rtt_estimator(sk, seq_rtt);
 	tcp_set_rto(sk);
+	TCP_ESTATS_UPDATE(tcp_sk(sk), tcp_estats_update_rtt(sk, seq_rtt));
 	inet_csk(sk)->icsk_backoff = 0;
 }
 
@@ -3175,6 +3202,13 @@ static inline void tcp_ack_update_rtt(struct sock *sk, const int flag,
 static void tcp_cong_avoid(struct sock *sk, u32 ack, u32 in_flight)
 {
 	const struct inet_connection_sock *icsk = inet_csk(sk);
+	struct tcp_sock *tp = tcp_sk(sk);
+
+	if (tp->snd_cwnd > tp->snd_cwnd_clamp) {
+		tp->snd_cwnd--;
+		return;
+	}
+
 	icsk->icsk_ca_ops->cong_avoid(sk, ack, in_flight);
 	tcp_sk(sk)->snd_cwnd_stamp = tcp_time_stamp;
 }
@@ -3463,9 +3497,11 @@ static int tcp_ack_update_window(struct sock *sk, struct sk_buff *skb, u32 ack,
 				tp->max_window = nwin;
 				tcp_sync_mss(sk, inet_csk(sk)->icsk_pmtu_cookie);
 			}
+			TCP_ESTATS_UPDATE(tp, tcp_estats_update_rwin_rcvd(tp));
 		}
 	}
 
+	TCP_ESTATS_UPDATE(tp, tcp_estats_update_acked(tp, ack));
 	tp->snd_una = ack;
 
 	return flag;
@@ -3607,6 +3643,7 @@ static int tcp_process_frto(struct sock *sk, int flag)
 		tp->frto_counter = 0;
 		tp->undo_marker = 0;
 		NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPSPURIOUSRTOS);
+		TCP_ESTATS_VAR_INC(tp, SpuriousRtoDetected);
 	}
 	return 0;
 }
@@ -3623,22 +3660,33 @@ static int tcp_ack(struct sock *sk, struct sk_buff *skb, int flag)
 	u32 prior_fackets;
 	int prior_packets;
 	int frto_cwnd = 0;
+	int prior_state = icsk->icsk_ca_state;
 
 	/* If the ack is older than previous acks
 	 * then we can probably ignore it.
 	 */
-	if (before(ack, prior_snd_una))
+	if (before(ack, prior_snd_una)) {
+		TCP_ESTATS_VAR_INC(tp, SoftErrors);
+		TCP_ESTATS_VAR_SET(tp, SoftErrorReason, 3);
 		goto old_ack;
+	}
 
 	/* If the ack includes data we haven't sent yet, discard
 	 * this segment (RFC793 Section 3.9).
 	 */
-	if (after(ack, tp->snd_nxt))
+	if (after(ack, tp->snd_nxt)) {
+		TCP_ESTATS_VAR_INC(tp, SoftErrors);
+		TCP_ESTATS_VAR_SET(tp, SoftErrorReason, 4);
 		goto invalid_ack;
+	}
 
-	if (after(ack, prior_snd_una))
+	if (after(ack, prior_snd_una)) {
 		flag |= FLAG_SND_UNA_ADVANCED;
-
+		if (icsk->icsk_ca_state == TCP_CA_Disorder)
+			TCP_ESTATS_VAR_ADD(tp, SumOctetsReordered,
+					   ack - prior_snd_una);
+	}
+	
 	if (sysctl_tcp_abc) {
 		if (icsk->icsk_ca_state < TCP_CA_CWR)
 			tp->bytes_acked += ack - prior_snd_una;
@@ -3657,6 +3705,7 @@ static int tcp_ack(struct sock *sk, struct sk_buff *skb, int flag)
 		 * Note, we use the fact that SND.UNA>=SND.WL2.
 		 */
 		tcp_update_wl(tp, ack_seq);
+		TCP_ESTATS_UPDATE(tp, tcp_estats_update_acked(tp, ack));
 		tp->snd_una = ack;
 		flag |= FLAG_WIN_UPDATE;
 
@@ -3706,6 +3755,9 @@ static int tcp_ack(struct sock *sk, struct sk_buff *skb, int flag)
 			tcp_cong_avoid(sk, ack, prior_in_flight);
 		tcp_fastretrans_alert(sk, prior_packets - tp->packets_out,
 				      flag);
+		if (icsk->icsk_ca_state == TCP_CA_Open &&
+		    prior_state >= TCP_CA_CWR)
+			TCP_ESTATS_UPDATE(tp, tcp_estats_update_post_congestion(tp));
 	} else {
 		if ((flag & FLAG_DATA_ACKED) && !frto_cwnd)
 			tcp_cong_avoid(sk, ack, prior_in_flight);
@@ -4337,6 +4389,7 @@ static void tcp_ofo_queue(struct sock *sk)
 
 		__skb_unlink(skb, &tp->out_of_order_queue);
 		__skb_queue_tail(&sk->sk_receive_queue, skb);
+		TCP_ESTATS_UPDATE(tp, tcp_estats_update_rcvd(tp, TCP_SKB_CB(skb)->end_seq));
 		tp->rcv_nxt = TCP_SKB_CB(skb)->end_seq;
 		if (tcp_hdr(skb)->fin)
 			tcp_fin(skb, sk, tcp_hdr(skb));
@@ -4417,6 +4470,7 @@ queue_and_out:
 			skb_set_owner_r(skb, sk);
 			__skb_queue_tail(&sk->sk_receive_queue, skb);
 		}
+		TCP_ESTATS_UPDATE(tp, tcp_estats_update_rcvd(tp, TCP_SKB_CB(skb)->end_seq));
 		tp->rcv_nxt = TCP_SKB_CB(skb)->end_seq;
 		if (skb->len)
 			tcp_event_data_recv(sk, skb);
@@ -4438,6 +4492,8 @@ queue_and_out:
 
 		tcp_fast_path_check(sk);
 
+		TCP_ESTATS_UPDATE(tp, tcp_estats_update_recvq(sk));
+
 		if (eaten > 0)
 			__kfree_skb(skb);
 		else if (!sock_flag(sk, SOCK_DEAD))
@@ -4492,8 +4548,10 @@ drop:
 	SOCK_DEBUG(sk, "out of order segment: rcv_next %X seq %X - %X\n",
 		   tp->rcv_nxt, TCP_SKB_CB(skb)->seq, TCP_SKB_CB(skb)->end_seq);
 
+	TCP_ESTATS_UPDATE(tp, tcp_estats_update_recvq(sk));
 	skb_set_owner_r(skb, sk);
 
+	TCP_ESTATS_VAR_INC(tp, DupAcksOut);
 	if (!skb_peek(&tp->out_of_order_queue)) {
 		/* Initial out of order segment, build 1 SACK. */
 		if (tcp_is_sack(tp)) {
@@ -4503,6 +4561,7 @@ drop:
 						TCP_SKB_CB(skb)->end_seq;
 		}
 		__skb_queue_head(&tp->out_of_order_queue, skb);
+		TCP_ESTATS_VAR_INC(tp, DupAckEpisodes);
 	} else {
 		struct sk_buff *skb1 = skb_peek_tail(&tp->out_of_order_queue);
 		u32 seq = TCP_SKB_CB(skb)->seq;
@@ -4845,6 +4904,8 @@ void tcp_cwnd_application_limited(struct sock *sk)
 		if (win_used < tp->snd_cwnd) {
 			tp->snd_ssthresh = tcp_current_ssthresh(sk);
 			tp->snd_cwnd = (tp->snd_cwnd + win_used) >> 1;
+			TCP_ESTATS_VAR_INC(tp, OtherReductions);
+			TCP_ESTATS_VAR_INC(tp, OtherReductionsCV);
 		}
 		tp->snd_cwnd_used = 0;
 	}
@@ -5158,6 +5219,8 @@ static int tcp_validate_incoming(struct sock *sk, struct sk_buff *skb,
 	    tcp_paws_discard(sk, skb)) {
 		if (!th->rst) {
 			NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_PAWSESTABREJECTED);
+			TCP_ESTATS_VAR_INC(tp, SoftErrors);
+			TCP_ESTATS_VAR_SET(tp, SoftErrorReason, 5);
 			tcp_send_dupack(sk, skb);
 			goto discard;
 		}
@@ -5174,6 +5237,10 @@ static int tcp_validate_incoming(struct sock *sk, struct sk_buff *skb,
 		 */
 		if (!th->rst)
 			tcp_send_dupack(sk, skb);
+		TCP_ESTATS_VAR_INC(tp, SoftErrors);
+		TCP_ESTATS_VAR_SET(tp, SoftErrorReason,
+				   before(TCP_SKB_CB(skb)->end_seq, tp->rcv_wup) ?
+				   1 : 2);
 		goto discard;
 	}
 
@@ -5309,6 +5376,8 @@ int tcp_rcv_established(struct sock *sk, struct sk_buff *skb,
 				return 0;
 			} else { /* Header too small */
 				TCP_INC_STATS_BH(sock_net(sk), TCP_MIB_INERRS);
+				TCP_ESTATS_VAR_INC(tp, SoftErrors);
+				TCP_ESTATS_VAR_SET(tp, SoftErrorReason, 8);
 				goto discard;
 			}
 		} else {
@@ -5344,6 +5413,7 @@ int tcp_rcv_established(struct sock *sk, struct sk_buff *skb,
 					tcp_rcv_rtt_measure_ts(sk, skb);
 
 					__skb_pull(skb, tcp_header_len);
+					TCP_ESTATS_UPDATE(tp, tcp_estats_update_rcvd(tp, TCP_SKB_CB(skb)->end_seq));
 					tp->rcv_nxt = TCP_SKB_CB(skb)->end_seq;
 					NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPHPHITSTOUSER);
 				}
@@ -5374,9 +5444,11 @@ int tcp_rcv_established(struct sock *sk, struct sk_buff *skb,
 				__skb_pull(skb, tcp_header_len);
 				__skb_queue_tail(&sk->sk_receive_queue, skb);
 				skb_set_owner_r(skb, sk);
+				TCP_ESTATS_UPDATE(tp, tcp_estats_update_rcvd(tp, TCP_SKB_CB(skb)->end_seq));
 				tp->rcv_nxt = TCP_SKB_CB(skb)->end_seq;
 			}
 
+			TCP_ESTATS_UPDATE(tp, tcp_estats_update_recvq(sk));
 			tcp_event_data_recv(sk, skb);
 
 			if (TCP_SKB_CB(skb)->ack_seq != tp->snd_una) {
@@ -5433,6 +5505,8 @@ step5:
 
 csum_error:
 	TCP_INC_STATS_BH(sock_net(sk), TCP_MIB_INERRS);
+	TCP_ESTATS_VAR_INC(tp, SoftErrors);
+	TCP_ESTATS_VAR_SET(tp, SoftErrorReason, 7);
 
 discard:
 	__kfree_skb(skb);
@@ -5573,6 +5647,7 @@ static int tcp_rcv_synsent_state_process(struct sock *sk, struct sk_buff *skb,
 
 		smp_mb();
 		tcp_set_state(sk, TCP_ESTABLISHED);
+		tcp_estats_establish(sk);
 
 		security_inet_conn_established(sk, skb);
 
@@ -5791,6 +5866,7 @@ int tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb,
 				smp_mb();
 				tcp_set_state(sk, TCP_ESTABLISHED);
 				sk->sk_state_change(sk);
+				tcp_estats_establish(sk);
 
 				/* Note, that this wakeup is only for marginal
 				 * crossed SYN case. Passively open sockets
diff --git a/net/ipv4/tcp_ipv4.c b/net/ipv4/tcp_ipv4.c
index 708dc20..e70bb1c 100644
--- a/net/ipv4/tcp_ipv4.c
+++ b/net/ipv4/tcp_ipv4.c
@@ -230,7 +230,10 @@ int tcp_v4_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)
 		inet_csk(sk)->icsk_ext_hdr_len = inet_opt->opt.optlen;
 
 	tp->rx_opt.mss_clamp = TCP_MSS_DEFAULT;
-
+#ifdef CONFIG_TCP_ESTATS
+	tp->rx_opt.rec_mss = 0;
+#endif
+	
 	/* Socket identity is still unknown (sport may be zero).
 	 * However we set state to SYN-SENT and not releasing socket
 	 * lock select source port, enter ourselves into the hash tables and
@@ -257,6 +260,8 @@ int tcp_v4_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)
 							   inet->inet_daddr,
 							   inet->inet_sport,
 							   usin->sin_port);
+	TCP_ESTATS_VAR_SET(tp, SndInitial, tp->write_seq);
+	TCP_ESTATS_VAR_SET(tp, SndMax, tp->write_seq);
 
 	inet->inet_id = tp->write_seq ^ jiffies;
 
@@ -1278,6 +1283,7 @@ int tcp_v4_conn_request(struct sock *sk, struct sk_buff *skb)
 	tcp_clear_options(&tmp_opt);
 	tmp_opt.mss_clamp = TCP_MSS_DEFAULT;
 	tmp_opt.user_mss  = tp->rx_opt.user_mss;
+	tmp_opt.rec_mss = 0;
 	tcp_parse_options(skb, &tmp_opt, &hash_location, 0);
 
 	if (tmp_opt.cookie_plus > 0 &&
@@ -1427,6 +1433,8 @@ struct sock *tcp_v4_syn_recv_sock(struct sock *sk, struct sk_buff *skb,
 	if (!newsk)
 		goto exit_nonewsk;
 
+	tcp_estats_create(newsk, TCP_ESTATS_ADDRTYPE_IPV4);
+
 	newsk->sk_gso_type = SKB_GSO_TCPV4;
 
 	newtp		      = tcp_sk(newsk);
@@ -1690,6 +1698,7 @@ process:
 	skb->dev = NULL;
 
 	bh_lock_sock_nested(sk);
+	TCP_ESTATS_UPDATE(tcp_sk(sk), tcp_estats_update_segrecv(tcp_sk(sk), skb));
 	ret = 0;
 	if (!sock_owned_by_user(sk)) {
 #ifdef CONFIG_NET_DMA
@@ -1709,6 +1718,7 @@ process:
 		NET_INC_STATS_BH(net, LINUX_MIB_TCPBACKLOGDROP);
 		goto discard_and_relse;
 	}
+	TCP_ESTATS_UPDATE(tcp_sk(sk), tcp_estats_update_finish_segrecv(tcp_sk(sk)));
 	bh_unlock_sock(sk);
 
 	sock_put(sk);
@@ -1893,6 +1903,8 @@ static int tcp_v4_init_sock(struct sock *sk)
 	 */
 	sk->sk_sndbuf = sysctl_tcp_wmem[1];
 	sk->sk_rcvbuf = sysctl_tcp_rmem[1];
+	
+	tcp_estats_create(sk, TCP_ESTATS_ADDRTYPE_IPV4);
 
 	local_bh_disable();
 	percpu_counter_inc(&tcp_sockets_allocated);
@@ -1936,6 +1948,8 @@ void tcp_v4_destroy_sock(struct sock *sk)
 	if (inet_csk(sk)->icsk_bind_hash)
 		inet_put_port(sk);
 
+	tcp_estats_destroy(sk);
+
 	/*
 	 * If sendmsg cached page exists, toss it.
 	 */
diff --git a/net/ipv4/tcp_minisocks.c b/net/ipv4/tcp_minisocks.c
index 80b1f80..47d5182 100644
--- a/net/ipv4/tcp_minisocks.c
+++ b/net/ipv4/tcp_minisocks.c
@@ -548,6 +548,9 @@ struct sock *tcp_create_openreq_child(struct sock *sk, struct request_sock *req,
 		if (skb->len >= TCP_MSS_DEFAULT + newtp->tcp_header_len)
 			newicsk->icsk_ack.last_seg_size = skb->len - newtp->tcp_header_len;
 		newtp->rx_opt.mss_clamp = req->mss;
+#ifdef CONFIG_TCP_ESTATS
+		newtp->rx_opt.rec_mss = req->mss;
+#endif
 		TCP_ECN_openreq_child(newtp, req);
 
 		TCP_INC_STATS_BH(sock_net(sk), TCP_MIB_PASSIVEOPENS);
diff --git a/net/ipv4/tcp_output.c b/net/ipv4/tcp_output.c
index 882e0b0..26a8a11 100644
--- a/net/ipv4/tcp_output.c
+++ b/net/ipv4/tcp_output.c
@@ -72,6 +72,7 @@ static void tcp_event_new_data_sent(struct sock *sk, struct sk_buff *skb)
 
 	tcp_advance_send_head(sk, skb);
 	tp->snd_nxt = TCP_SKB_CB(skb)->end_seq;
+	TCP_ESTATS_UPDATE(tp, tcp_estats_update_snd_nxt(tp));
 
 	/* Don't override Nagle indefinitely with F-RTO */
 	if (tp->frto_counter == 2)
@@ -275,6 +276,7 @@ static u16 tcp_select_window(struct sock *sk)
 	}
 	tp->rcv_wnd = new_win;
 	tp->rcv_wup = tp->rcv_nxt;
+	TCP_ESTATS_UPDATE(tp, tcp_estats_update_rwin_sent(tp));
 
 	/* Make sure we do not exceed the maximum possible
 	 * scaled window.
@@ -895,15 +897,32 @@ static int tcp_transmit_skb(struct sock *sk, struct sk_buff *skb, int clone_it,
 	if (skb->len != tcp_header_size)
 		tcp_event_data_sent(tp, skb, sk);
 
-	if (after(tcb->end_seq, tp->snd_nxt) || tcb->seq == tcb->end_seq)
-		TCP_ADD_STATS(sock_net(sk), TCP_MIB_OUTSEGS,
-			      tcp_skb_pcount(skb));
+	{
+		/* If the skb isn't cloned, we can't reference it after
+		 * calling queue_xmit, so copy everything we need here. */
+		int pcount = tcp_skb_pcount(skb); 
+#ifdef CONFIG_TCP_ESTATS
+		int len = skb->len;
+		__u32 seq = TCP_SKB_CB(skb)->seq;
+		__u32 end_seq = TCP_SKB_CB(skb)->end_seq;
+		int flags = TCP_SKB_CB(skb)->flags;
+#endif
+
+		err = icsk->icsk_af_ops->queue_xmit(skb, &inet->cork.fl);
+		
+		if (err == 0) {
+			if (after(tcb->end_seq, tp->snd_nxt) || tcb->seq == tcb->end_seq)
+				TCP_ADD_STATS(sock_net(sk), TCP_MIB_OUTSEGS, pcount);
+			TCP_ESTATS_UPDATE(tp, tcp_estats_update_segsend(sk, len, pcount,
+									seq, end_seq, flags));
+		}
+	}
 
-	err = icsk->icsk_af_ops->queue_xmit(skb, &inet->cork.fl);
 	if (likely(err <= 0))
 		return err;
 
 	tcp_enter_cwr(sk, 1);
+	TCP_ESTATS_VAR_INC(tp, SendStall);
 
 	return net_xmit_eval(err);
 }
@@ -1240,6 +1259,7 @@ unsigned int tcp_sync_mss(struct sock *sk, u32 pmtu)
 	if (icsk->icsk_mtup.enabled)
 		mss_now = min(mss_now, tcp_mtu_to_mss(sk, icsk->icsk_mtup.search_low));
 	tp->mss_cache = mss_now;
+	TCP_ESTATS_UPDATE(tp, tcp_estats_update_mss(tp));
 
 	return mss_now;
 }
@@ -1434,7 +1454,7 @@ static inline int tcp_snd_wnd_test(struct tcp_sock *tp, struct sk_buff *skb,
  * should be put on the wire right now.  If so, it returns the number of
  * packets allowed by the congestion window.
  */
-static unsigned int tcp_snd_test(struct sock *sk, struct sk_buff *skb,
+static unsigned int tcp_snd_wait(struct sock *sk, struct sk_buff *skb,
 				 unsigned int cur_mss, int nonagle)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
@@ -1443,11 +1463,13 @@ static unsigned int tcp_snd_test(struct sock *sk, struct sk_buff *skb,
 	tcp_init_tso_segs(sk, skb, cur_mss);
 
 	if (!tcp_nagle_test(tp, skb, cur_mss, nonagle))
-		return 0;
+		return -TCP_ESTATS_SNDLIM_SENDER;
 
 	cwnd_quota = tcp_cwnd_test(tp, skb);
-	if (cwnd_quota && !tcp_snd_wnd_test(tp, skb, cur_mss))
-		cwnd_quota = 0;
+	if (!cwnd_quota)
+		return -TCP_ESTATS_SNDLIM_CWND;
+	if (!tcp_snd_wnd_test(tp, skb, cur_mss))
+		return -TCP_ESTATS_SNDLIM_RWIN;
 
 	return cwnd_quota;
 }
@@ -1459,9 +1481,9 @@ int tcp_may_send_now(struct sock *sk)
 	struct sk_buff *skb = tcp_send_head(sk);
 
 	return skb &&
-		tcp_snd_test(sk, skb, tcp_current_mss(sk),
+		tcp_snd_wait(sk, skb, tcp_current_mss(sk),
 			     (tcp_skb_is_last(sk, skb) ?
-			      tp->nonagle : TCP_NAGLE_PUSH));
+			      tp->nonagle : TCP_NAGLE_PUSH)) > 0;
 }
 
 /* Trim TSO SKB to LEN bytes, put the remaining data into a new packet
@@ -1740,6 +1762,7 @@ static int tcp_write_xmit(struct sock *sk, unsigned int mss_now, int nonagle,
 	unsigned int tso_segs, sent_pkts;
 	int cwnd_quota;
 	int result;
+	int why = TCP_ESTATS_SNDLIM_NONE;
 
 	sent_pkts = 0;
 
@@ -1760,20 +1783,28 @@ static int tcp_write_xmit(struct sock *sk, unsigned int mss_now, int nonagle,
 		BUG_ON(!tso_segs);
 
 		cwnd_quota = tcp_cwnd_test(tp, skb);
-		if (!cwnd_quota)
+		if (!cwnd_quota) {
+			why = TCP_ESTATS_SNDLIM_CWND;
 			break;
+		}
 
-		if (unlikely(!tcp_snd_wnd_test(tp, skb, mss_now)))
+		if (unlikely(!tcp_snd_wnd_test(tp, skb, mss_now))) {
+			why = TCP_ESTATS_SNDLIM_RWIN;
 			break;
+		}
 
 		if (tso_segs == 1) {
 			if (unlikely(!tcp_nagle_test(tp, skb, mss_now,
 						     (tcp_skb_is_last(sk, skb) ?
-						      nonagle : TCP_NAGLE_PUSH))))
+						      nonagle : TCP_NAGLE_PUSH)))) {
+				why = TCP_ESTATS_SNDLIM_SENDER;
 				break;
+			}
 		} else {
-			if (!push_one && tcp_tso_should_defer(sk, skb))
+			if (!push_one && tcp_tso_should_defer(sk, skb)) {
+				why = TCP_ESTATS_SNDLIM_CWND;
 				break;
+			}
 		}
 
 		limit = mss_now;
@@ -1782,13 +1813,17 @@ static int tcp_write_xmit(struct sock *sk, unsigned int mss_now, int nonagle,
 						    cwnd_quota);
 
 		if (skb->len > limit &&
-		    unlikely(tso_fragment(sk, skb, limit, mss_now, gfp)))
+		    unlikely(tso_fragment(sk, skb, limit, mss_now, gfp))) {
+			why = TCP_ESTATS_SNDLIM_SENDER;
 			break;
+		}
 
 		TCP_SKB_CB(skb)->when = tcp_time_stamp;
 
-		if (unlikely(tcp_transmit_skb(sk, skb, 1, gfp)))
+		if (unlikely(tcp_transmit_skb(sk, skb, 1, gfp))) {
+			why = TCP_ESTATS_SNDLIM_SENDER;
 			break;
+		}
 
 		/* Advance the send_head.  This one is sent out.
 		 * This call will increment packets_out.
@@ -1801,6 +1836,9 @@ static int tcp_write_xmit(struct sock *sk, unsigned int mss_now, int nonagle,
 		if (push_one)
 			break;
 	}
+	if (why == TCP_ESTATS_SNDLIM_NONE)
+		why = TCP_ESTATS_SNDLIM_SENDER;
+	TCP_ESTATS_UPDATE(tp, tcp_estats_update_sndlim(tp, why));
 
 	if (likely(sent_pkts)) {
 		tcp_cwnd_validate(sk);
@@ -2640,6 +2678,7 @@ int tcp_connect(struct sock *sk)
 	 * in order to make this packet get counted in tcpOutSegs.
 	 */
 	tp->snd_nxt = tp->write_seq;
+	TCP_ESTATS_UPDATE(tp, tcp_estats_update_snd_nxt(tp));
 	tp->pushed_seq = tp->write_seq;
 	TCP_INC_STATS(sock_net(sk), TCP_MIB_ACTIVEOPENS);
 
diff --git a/net/ipv4/tcp_timer.c b/net/ipv4/tcp_timer.c
index ecd44b0..22a7a85 100644
--- a/net/ipv4/tcp_timer.c
+++ b/net/ipv4/tcp_timer.c
@@ -304,6 +304,7 @@ static void tcp_probe_timer(struct sock *sk)
 		if (tcp_out_of_resources(sk, alive || icsk->icsk_probes_out <= max_probes))
 			return;
 	}
+	TCP_ESTATS_UPDATE(tp, tcp_estats_update_timeout(sk));
 
 	if (icsk->icsk_probes_out > max_probes) {
 		tcp_write_err(sk);
diff --git a/net/ipv6/tcp_ipv6.c b/net/ipv6/tcp_ipv6.c
index 87551ca..2a8a81f 100644
--- a/net/ipv6/tcp_ipv6.c
+++ b/net/ipv6/tcp_ipv6.c
@@ -299,6 +299,9 @@ static int tcp_v6_connect(struct sock *sk, struct sockaddr *uaddr,
 					  np->opt->opt_nflen);
 
 	tp->rx_opt.mss_clamp = IPV6_MIN_MTU - sizeof(struct tcphdr) - sizeof(struct ipv6hdr);
+#ifdef CONFIG_TCP_ESTATS
+	tp->rx_opt.rec_mss = 0;
+#endif
 
 	inet->inet_dport = usin->sin6_port;
 
@@ -313,6 +316,9 @@ static int tcp_v6_connect(struct sock *sk, struct sockaddr *uaddr,
 							     inet->inet_sport,
 							     inet->inet_dport);
 
+	TCP_ESTATS_VAR_SET(tp, SndInitial, tp->write_seq);
+	TCP_ESTATS_VAR_SET(tp, SndMax, tp->write_seq);
+
 	err = tcp_connect(sk);
 	if (err)
 		goto late_failure;
@@ -1215,6 +1221,7 @@ static int tcp_v6_conn_request(struct sock *sk, struct sk_buff *skb)
 	tcp_clear_options(&tmp_opt);
 	tmp_opt.mss_clamp = IPV6_MIN_MTU - sizeof(struct tcphdr) - sizeof(struct ipv6hdr);
 	tmp_opt.user_mss = tp->rx_opt.user_mss;
+	tmp_opt.rec_mss = 0;
 	tcp_parse_options(skb, &tmp_opt, &hash_location, 0);
 
 	if (tmp_opt.cookie_plus > 0 &&
@@ -1442,6 +1449,8 @@ static struct sock * tcp_v6_syn_recv_sock(struct sock *sk, struct sk_buff *skb,
 	if (newsk == NULL)
 		goto out_nonewsk;
 
+	tcp_estats_create(newsk, TCP_ESTATS_ADDRTYPE_IPV6);
+
 	/*
 	 * No need to charge this sock to the relevant IPv6 refcnt debug socks
 	 * count here, tcp_create_openreq_child now does this for us, see the
@@ -1761,6 +1770,7 @@ process:
 	skb->dev = NULL;
 
 	bh_lock_sock_nested(sk);
+	TCP_ESTATS_UPDATE(tcp_sk(sk), tcp_estats_update_segrecv(tcp_sk(sk), skb));
 	ret = 0;
 	if (!sock_owned_by_user(sk)) {
 #ifdef CONFIG_NET_DMA
@@ -1780,6 +1790,7 @@ process:
 		NET_INC_STATS_BH(net, LINUX_MIB_TCPBACKLOGDROP);
 		goto discard_and_relse;
 	}
+	TCP_ESTATS_UPDATE(tcp_sk(sk), tcp_estats_update_finish_segrecv(tcp_sk(sk)));
 	bh_unlock_sock(sk);
 
 	sock_put(sk);
@@ -2004,6 +2015,8 @@ static int tcp_v6_init_sock(struct sock *sk)
 	 */
 	sk->sk_sndbuf = sysctl_tcp_wmem[1];
 	sk->sk_rcvbuf = sysctl_tcp_rmem[1];
+	
+	tcp_estats_create(sk, TCP_ESTATS_ADDRTYPE_IPV6);
 
 	local_bh_disable();
 	percpu_counter_inc(&tcp_sockets_allocated);
